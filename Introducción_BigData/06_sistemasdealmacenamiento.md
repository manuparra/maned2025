# ğŸ“Œ Sistemas de Almacenamiento para Big Data en R y Apache Spark

Sistemas de almacenamiento mÃ¡s populares utilizados para manejar grandes volÃºmenes de datos, centrÃ¡ndonos en la integraciÃ³n con **R** mediante **Apache Spark** (`sparklyr`). Hablaremos de **HDFS**, **Amazon S3** y **Google Cloud Storage**, explicando sus modelos de distribuciÃ³n, ventajas y ejemplos reales de acceso a datos.

---

## ğŸ”¹ **1. Hadoop Distributed File System (HDFS)**

### ğŸ“‚ **Â¿QuÃ© es HDFS?**
**HDFS** es un sistema de archivos distribuido diseÃ±ado para almacenar grandes conjuntos de datos, distribuidos en varios nodos de un clÃºster. Se basa en la replicaciÃ³n de bloques para garantizar tolerancia a fallos y acceso paralelo eficiente.

### ğŸ”¸ **Modelo de DistribuciÃ³n**
- Los datos se dividen en bloques (generalmente de 128 MB).
- Los bloques se replican en varios nodos (por defecto 3 copias).
- El acceso a datos es paralelo y local a los nodos que los almacenan, mejorando el rendimiento.

### ğŸ”¸ **Acceso a HDFS desde R con Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "yarn")

# Leer datos desde HDFS
datos <- spark_read_csv(sc, name = "datos_hdfs", path = "hdfs://cluster/user/data/sample_data.csv")

# Ejemplo real: dataset pÃºblico de la ciudad de Nueva York (Taxi trips)
# Ruta de ejemplo real (pÃºblica en muchos clÃºsteres):
ruta_hdfs <- "hdfs://cluster/public/nyc_taxi/yellow_tripdata_2023-01.csv"
taxi_nyc <- spark_read_csv(sc, name = "taxi_nyc", path = ruta_hdfs, infer_schema = TRUE)

head(taxi_nyc)
```

### ğŸ”¸ **Ventajas de HDFS**
âœ… Alto rendimiento para grandes volÃºmenes de datos en paralelo.  
âœ… Alta tolerancia a fallos mediante replicaciÃ³n.  
âœ… Compatible nativamente con ecosistemas Hadoop/Spark.

---

## ğŸ”¹ **2. Amazon S3 (Simple Storage Service)**

### ğŸ“‚ **Â¿QuÃ© es Amazon S3?**
Amazon S3 es un sistema de almacenamiento basado en objetos, ampliamente utilizado para almacenar datos en la nube, accesible globalmente y escalable sin lÃ­mites prÃ¡cticos.

### ğŸ”¸ **Modelo de DistribuciÃ³n**
- Almacenamiento basado en objetos distribuidos globalmente.
- Escalabilidad automÃ¡tica y acceso paralelo desde mÃºltiples clÃºsteres.
- IntegraciÃ³n directa con Spark y otras herramientas analÃ­ticas.

### ğŸ”¸ **Acceso a S3 desde R con Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

# Configurar acceso con credenciales AWS (se asume configuraciÃ³n previa)
# Sys.setenv(AWS_ACCESS_KEY_ID="tu_key", AWS_SECRET_ACCESS_KEY="tu_secret")

# Leer datos directamente desde Amazon S3
datos_s3 <- spark_read_parquet(sc, name = "datos_s3",
                               path = "s3a://mi-bucket/datos.parquet")

# Ejemplo real: dataset pÃºblico de Landsat en AWS
ruta_s3_landsat <- "s3a://landsat-pds/c1/L8/001/003/LC08_L1TP_001003_20170428_20170510_01_T1/*MTL.txt"
landsat <- spark_read_text(sc, name = "landsat_metadata", path = ruta_s3_landsat)

head(landsat)
```

### ğŸ”¸ **Ventajas de Amazon S3**
âœ… Alta escalabilidad sin mantenimiento del sistema de almacenamiento.  
âœ… IntegraciÃ³n nativa con AWS EMR y Spark.  
âœ… Pago segÃºn uso, con almacenamiento altamente duradero y disponible.

---

## ğŸ”¹ **3. Google Cloud Storage (GCS)**

### ğŸ“‚ **Â¿QuÃ© es Google Cloud Storage?**
Google Cloud Storage es un servicio de almacenamiento de objetos escalable y seguro en la nube de Google, diseÃ±ado para grandes volÃºmenes de datos.

### ğŸ”¸ **Modelo de DistribuciÃ³n**
- Objetos almacenados en "buckets" distribuidos geogrÃ¡ficamente.
- Alta disponibilidad y rendimiento mediante almacenamiento distribuido.
- Acceso eficiente mediante Spark y BigQuery.

### ğŸ”¸ **Acceso a Google Cloud Storage desde R con Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

# AutenticaciÃ³n previa con Google Cloud SDK o mediante archivo JSON de credenciales
# Sys.setenv(GOOGLE_APPLICATION_CREDENTIALS="ruta/credenciales.json")

# Leer datos CSV desde GCS
datos_gcs <- spark_read_csv(sc, name = "datos_gcs",
                            path = "gs://mi-bucket/data/dataset.csv")

# Ejemplo real: Dataset pÃºblico de COVID-19 en GCS
ruta_covid_gcs <- "gs://covid19-open-data/v2/latest/epidemiology.csv"
covid_data <- spark_read_csv(sc, name = "covid_data", path = ruta_covid_gcs, infer_schema = TRUE)

head(covid_data)
```

### ğŸ”¸ **Ventajas de Google Cloud Storage**
âœ… FÃ¡cil integraciÃ³n con Google BigQuery y servicios analÃ­ticos.  
âœ… Muy alto rendimiento para consultas de grandes datasets desde Spark.  
âœ… Seguridad avanzada y excelente gestiÃ³n del ciclo de vida de los datos.

---

## ğŸ”¹ **4. Comparativa de Sistemas de Almacenamiento**

| CaracterÃ­stica        | HDFS                          | Amazon S3                    | Google Cloud Storage   |
|-----------------------|-------------------------------|------------------------------|------------------------|
| **Tipo de Almacenamiento**  | Sistema distribuido en bloques | Basado en objetos            | Basado en objetos      |
| **Modelo de distribuciÃ³n**  | ReplicaciÃ³n en nodos          | DistribuciÃ³n global          | DistribuciÃ³n global    |
| **Velocidad de acceso**     | Alta (localidad en nodos)     | Alta (acceso paralelo global)| Alta (acceso paralelo global) |
| **IntegraciÃ³n con Spark**   | Nativa                        | Muy buena                    | Muy buena              |
| **Costos**                  | Requiere mantenimiento local   | Pago por uso (sin gestiÃ³n)   | Pago por uso (sin gestiÃ³n)  |
| **Ejemplos de uso real**    | Datos empresariales grandes, logs | Datos pÃºblicos (NASA, Landsat)| Datos pÃºblicos (COVID-19, NOAA)|

---

## ğŸ”¹ **5. Modelo de DistribuciÃ³n Local en Nodos**
- **Spark** prioriza el acceso paralelo aprovechando el principio de "data locality": los cÃ¡lculos se realizan lo mÃ¡s cerca posible del nodo donde residen fÃ­sicamente los datos.
- Los datos almacenados en sistemas distribuidos (como HDFS) permiten que Spark ejecute operaciones de forma eficiente, minimizando el movimiento de datos a travÃ©s de la red.

**Ejemplo grÃ¡fico simplificado**:
```
Nodo 1      Nodo 2      Nodo 3
[datos] --> [datos] --> [datos]
  â”‚           â”‚           â”‚
[Spark]     [Spark]     [Spark]
```

Cada nodo procesa simultÃ¡neamente los datos locales, aumentando la eficiencia del acceso paralelo.

---

AquÃ­ tienes el documento ampliado con una secciÃ³n adicional sobre **tipos de ficheros mÃ¡s utilizados en Big Data** y cÃ³mo trabajar con ellos desde **R y Apache Spark**.

---

# ğŸ“Œ **Tipos de Ficheros mÃ¡s usados en Big Data con R y Spark**

En Big Data, la elecciÃ³n del formato del fichero es fundamental para optimizar el almacenamiento, acceso y procesamiento eficiente de grandes conjuntos de datos. A continuaciÃ³n, revisamos los formatos mÃ¡s comunes, sus caracterÃ­sticas, ventajas y ejemplos de uso con R y Spark.

---

## ğŸ”¹ **1. CSV (Comma Separated Values)**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato basado en texto plano.
- Alta compatibilidad y fÃ¡cil manejo.
- Poco eficiente para grandes volÃºmenes de datos (no comprimido, sin indexado).

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

datos_csv <- spark_read_csv(sc, name = "datos_csv", path = "hdfs://ruta/datos.csv")

# Ejemplo pÃºblico: NYC Taxi Dataset (CSV)
ruta_nyc <- "s3a://nyc-tlc/trip data/yellow_tripdata_2023-01.csv"
nyc_taxi <- spark_read_csv(sc, "nyc_taxi", ruta_nyc, infer_schema = TRUE)

head(nyc_taxi)
```

---

## ğŸ”¹ **2. Parquet**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato binario columnar altamente eficiente.
- CompresiÃ³n efectiva y soporte para particionado.
- Ideal para anÃ¡lisis rÃ¡pidos sobre columnas especÃ­ficas.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

datos_parquet <- spark_read_parquet(sc, name = "datos_parquet", path = "s3a://mi-bucket/datos.parquet")

# Ejemplo pÃºblico: Amazon Reviews (Parquet)
ruta_reviews <- "s3a://amazon-reviews-pds/parquet/product_category=Electronics/*.parquet"
reviews <- spark_read_parquet(sc, "amazon_reviews", ruta_reviews)

head(reviews)
```

---

## ğŸ”¹ **3. ORC (Optimized Row Columnar)**

### ğŸ“‚ **CaracterÃ­sticas**
- Similar a Parquet (almacenamiento columnar comprimido).
- Mejor rendimiento en consultas complejas de Hive.
- Muy usado en ecosistemas Hadoop.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "yarn")

datos_orc <- spark_read_orc(sc, name = "datos_orc", path = "hdfs://ruta/datos.orc")

# Ejemplo real (datos de ventas almacenados en Hadoop)
ruta_ventas <- "hdfs://cluster/user/data/ventas.orc"
ventas <- spark_read_orc(sc, "ventas", ruta_ventas)

head(ventas)
```

---

## ğŸ”¹ **4. Avro**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato basado en filas con esquema definido.
- Muy eficiente en sistemas de transmisiÃ³n (Kafka).
- Compatible con esquemas evolutivos.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

# Spark nativamente puede necesitar paquetes adicionales para leer Avro
datos_avro <- spark_read_avro(sc, name = "datos_avro", path = "s3a://ruta/datos.avro")

# Ejemplo pÃºblico: datos climÃ¡ticos en formato Avro
ruta_clima <- "s3a://noaa-avro-pds/datos-clima/*.avro"
clima <- spark_read_avro(sc, "clima", ruta_clima)

head(clima)
```

---

## ğŸ”¹ **5. JSON**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato texto, estructurado, ligero y fÃ¡cil de leer.
- Menos eficiente en Big Data, aunque comÃºn en datos semi-estructurados.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

datos_json <- spark_read_json(sc, name = "datos_json", path = "gs://bucket/datos.json")

# Ejemplo real: datos abiertos de Twitter (JSON)
ruta_twitter <- "gs://public-datasets/twitter_sample/*.json"
tweets <- spark_read_json(sc, "tweets", ruta_twitter)

head(tweets)
```

---

## ğŸ”¹ **6. Feather (Apache Arrow)**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato ligero diseÃ±ado para intercambio rÃ¡pido entre R y Python.
- Alto rendimiento para anÃ¡lisis interactivos.

### ğŸ“Œ **Ejemplo en R**
```r
library(arrow)
library(dplyr)

datos_feather <- read_feather("datos.feather")

head(datos_feather)
```

(No disponible directamente en Spark, pero Ãºtil en entornos hÃ­bridos R/Python)

---

## ğŸ”¹ **7. Delta Lake**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato basado en Parquet con transacciones ACID.
- Soporte para versionado de datos.
- Integrado profundamente con Spark (Databricks).

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local", packages = "io.delta:delta-core_2.12:2.4.0")

datos_delta <- spark_read_delta(sc, name = "datos_delta", path = "s3a://bucket/delta-table/")

head(datos_delta)
```

---

## ğŸ”¹ **Comparativa de Formatos para Big Data**

| Formato | CompresiÃ³n | Rendimiento Lectura | Uso tÃ­pico | IntegraciÃ³n Spark |
|---------|------------|---------------------|------------|-------------------|
| CSV     | âŒ Baja    | âš ï¸ Medio-Bajo        | Datos abiertos, fÃ¡cil uso | âœ… Completa |
| Parquet | âœ… Alta    | ğŸš€ Alto (columnar)   | AnalÃ­tica eficiente | âœ… Completa |
| ORC     | âœ… Alta    | ğŸš€ Alto (columnar)   | Hadoop/Hive  | âœ… Completa |
| Avro    | âœ… Alta    | âš ï¸ Medio-Alto        | Streaming, intercambio estructurado | âœ… Completa |
| JSON    | âš ï¸ Medio   | âš ï¸ Medio-Bajo        | Semi-estructurados, APIs | âœ… Completa |
| Feather | âš ï¸ Medio-Alto | ğŸš€ Muy Alto       | AnÃ¡lisis rÃ¡pido (R-Python) | âŒ Indirecta |
| Delta Lake | âœ… Alta | ğŸš€ Muy Alto (versionado)| Data lakes avanzados | âœ… Completa |


## ğŸ”· Herramientas de Procesamiento de Datos en Streaming

Las herramientas de procesamiento de datos en streaming son tecnologÃ­as de software que permiten el procesamiento y anÃ¡lisis de flujos de datos en tiempo real. Estas herramientas permiten que las empresas obtengan informaciÃ³n y tomen decisiones a medida que se generan los datos, en lugar de esperar a que se recopilen y almacenen.

El procesamiento de datos en streaming generalmente utiliza sistemas de computaciÃ³n distribuida, donde los datos se procesan en mÃºltiples nodos dentro de un clÃºster. Esto garantiza un procesamiento de alto rendimiento y una alta escalabilidad.

### ğŸ”¹ Principales herramientas para el procesamiento de datos en streaming

Existen diversas herramientas diseÃ±adas para manejar datos en tiempo real. A continuaciÃ³n se presentan algunas de las mÃ¡s populares:

#### ğŸŸ  Apache Kafka

Kafka es una plataforma de streaming distribuida que permite el procesamiento de flujos de datos en tiempo real.

âœ… EstÃ¡ diseÃ±ado para manejar grandes volÃºmenes de datos de manera escalable y tolerante a fallos.
âœ… Kafka es ampliamente utilizado para casos de uso como monitorizaciÃ³n en tiempo real, procesamiento de logs, anÃ¡lisis de transacciones financieras y mÃ¡s.

Ejemplo de caso de uso:

    Empresas de telecomunicaciones utilizan Kafka para detectar interrupciones en la red en tiempo real.
    Plataformas de comercio electrÃ³nico lo emplean para rastrear eventos de usuario en su web.

#### ğŸŸ  Apache Flink

Apache Flink es un marco de procesamiento de datos de cÃ³digo abierto que admite tanto el procesamiento en batch como el procesamiento en streaming.

âœ… Proporciona APIs avanzadas para la creaciÃ³n de aplicaciones de streaming.
âœ… Flink se destaca por su capacidad para manejar datos a altas velocidades con una latencia muy baja.
âœ… Permite realizar anÃ¡lisis complejos en tiempo real, como la detecciÃ³n de anomalÃ­as o el seguimiento de patrones de comportamiento.

Ejemplo de caso de uso:

    Flink se utiliza en el sector financiero para detectar patrones de fraude en tiempo real.

#### ğŸŸ  Apache Storm

Apache Storm es un sistema distribuido para el procesamiento de flujos de datos a gran escala en tiempo real.

âœ… Ofrece un modelo de programaciÃ³n flexible para la construcciÃ³n de flujos de datos complejos.
âœ… DiseÃ±ado para ser escalable y tolerante a fallos, lo que garantiza la continuidad del procesamiento incluso en caso de errores en los nodos del clÃºster.

Ejemplo de caso de uso:

    Storm se emplea para analizar datos de redes sociales en tiempo real y detectar tendencias emergentes.

#### ğŸŸ  AWS Kinesis

Amazon Kinesis es una plataforma en la nube para el procesamiento de flujos de datos en tiempo real a gran escala.

âœ… Permite la ingestiÃ³n, el procesamiento y el anÃ¡lisis de datos en tiempo real.
âœ… Se integra fÃ¡cilmente con otros servicios de AWS para construir aplicaciones completas de streaming.
âœ… Es ideal para flujos de datos procedentes de dispositivos IoT, registros de aplicaciones web o sistemas de monitorizaciÃ³n en la nube.

Ejemplo de caso de uso:

    Kinesis se utiliza para procesar flujos de datos de sensores en tiempo real, lo que permite controlar maquinaria industrial o sistemas de climatizaciÃ³n inteligente.

#### ğŸŸ  Google Cloud Dataflow

Google Cloud Dataflow es un servicio totalmente gestionado que admite tanto el procesamiento por lotes como el procesamiento en streaming de conjuntos de datos a gran escala.

âœ… Soporta mÃºltiples lenguajes de programaciÃ³n como Java, Python, y Scala.
âœ… Ofrece una interfaz visual que facilita la creaciÃ³n, gestiÃ³n y monitorizaciÃ³n de pipelines de datos.
âœ… Su arquitectura escalable le permite manejar enormes cantidades de datos con una latencia mÃ­nima.

Ejemplo de caso de uso:

    Dataflow se utiliza para procesar datos de registros en tiempo real y alimentar sistemas de alerta temprana ante fallos en servidores.

### ğŸ”¹ ComparaciÃ³n de Herramientas de Procesamiento en Streaming
Herramienta	CaracterÃ­sticas clave	Escalabilidad	Tolerancia a fallos	IntegraciÃ³n con la nube
Apache Kafka	Plataforma robusta para ingesta de datos en tiempo real	â­â­â­â­â­	âœ… Alta	âŒ No gestionado
Apache Flink	Procesamiento rÃ¡pido y de baja latencia con potente API	â­â­â­â­	âœ… Alta	âŒ No gestionado
Apache Storm	Flexible, diseÃ±ado para flujos de datos complejos	â­â­â­â­	âœ… Alta	âŒ No gestionado
AWS Kinesis	IntegraciÃ³n con el ecosistema de Amazon Web Services	â­â­â­â­	âœ… Alta	âœ… Gestionado
Google Dataflow	GestiÃ³n visual de pipelines de datos en la nube	â­â­â­â­â­	âœ… Alta	âœ… Gestionado

Muy usadas en:

- âœ… Finanzas (detecciÃ³n de fraudes en tiempo real)
- âœ… Telecomunicaciones (monitorizaciÃ³n de redes)
- âœ… E-commerce (anÃ¡lisis de comportamiento del cliente)
- âœ… IoT (gestiÃ³n de datos de sensores en tiempo real)
