# 游늷 Instalaci칩n y Configuraci칩n de Apache Spark para R

---

## 游댳 **1. Instalaci칩n de Spark en R**
Para instalar Spark en R, abre **RStudio o Jupyter Lab** y ejecuta:

```r
# Instalar la librer칤a sparklyr
install.packages("sparklyr")

# Instalar Apache Spark en local (descarga autom치tica)
sparklyr::spark_install()
```

## Installaci칩n en Colab

```
install.packages("sparklyr")
library(sparklyr)
spark_available_versions()
park_install(version = "3.1")
sc <- spark_connect(master = "local", version = "3.1")
```


### **游늷 Notas sobre la instalaci칩n**
- `sparklyr` permite la conexi칩n entre **R y Apache Spark**.
- `spark_install()` descarga e instala Spark autom치ticamente en el sistema.

Si prefieres instalar **una versi칩n espec칤fica** de Spark, usa:

```r
sparklyr::spark_install(version = "3.2.0")
```

---

## 游댳 **2. Verificaci칩n de la instalaci칩n**
Para asegurarte de que Spark est치 correctamente instalado, ejecuta:

```r
library(sparklyr)
spark_available_versions()
```

Para comprobar que Spark se inicia correctamente, conecta R con Spark:

```r
sc <- spark_connect(master = "local")
sc
```

Si la conexi칩n es exitosa, ver치s informaci칩n sobre el servidor Spark.

---

## 游댳 **3. Configuraci칩n avanzada de Spark**
Si necesitas asignar m치s recursos a Spark, puedes personalizar la inicializaci칩n:

```r
sc <- spark_connect(master = "local", 
                    config = list(spark.memory.fraction = 0.8, 
                                  spark.executor.memory = "4G"))
```

O si est치s trabajando con **Spark en un cl칰ster**, cambia el `master`:

```r
sc <- spark_connect(master = "yarn")  # Para entornos Hadoop/YARN
sc <- spark_connect(master = "spark://ip-servidor:7077")  # Para Spark independiente
```

Aqu칤 tienes algunos **ejemplos pr치cticos** de uso de **Big Data en nubes p칰blicas** utilizando las herramientas mencionadas anteriormente. Estos ejemplos est치n dise침ados para ilustrar flujos de trabajo comunes en entornos reales.

---

游늷 Ejemplos de Uso de sparklyr en Nubes P칰blicas

### 游댳 **3.1. AWS - Procesamiento de Datos Masivos con Amazon EMR**
**Caso de uso:** Procesar grandes vol칰menes de logs de servidores web para obtener patrones de tr치fico.

#### 游댳 **Flujo de trabajo en AWS**
1. **Almacenar los logs en Amazon S3**.
2. Crear un cl칰ster de **Amazon EMR** con Apache Spark.
3. Conectar R a Spark mediante `sparklyr`.
4. Ejecutar consultas para identificar las p치ginas web m치s visitadas.

#### **C칩digo en R usando `sparklyr` en AWS EMR**
```r
library(sparklyr)
library(dplyr)

# Conectar a Spark en EMR
sc <- spark_connect(master = "yarn")

# Leer datos directamente desde S3
logs <- spark_read_csv(sc, name = "logs", path = "s3://mi-bucket/logs.csv")

# Procesamiento de datos en Spark
logs %>%
  filter(status == 200) %>%
  group_by(url) %>%
  summarise(visitas = n()) %>%
  arrange(desc(visitas)) %>%
  collect()  # Devuelve el resultado a R
```

---

### 游댳 **3.2. Google Cloud - An치lisis de Datos con BigQuery**
**Caso de uso:** Realizar an치lisis exploratorio de un conjunto de datos p칰blico con millones de registros.

#### 游댳 **Flujo de trabajo en Google Cloud**
1. Crear un proyecto en **Google Cloud** y habilitar BigQuery.
2. Subir los datos a **Google Cloud Storage** o utilizar un dataset p칰blico.
3. Utilizar la librer칤a `bigrquery` en R para conectarse a BigQuery.

#### **C칩digo en R usando `bigrquery`**
```r
library(bigrquery)
library(dplyr)

# Definir par치metros de conexi칩n
project_id <- "mi-proyecto"
sql <- "SELECT year, AVG(magnitude) as avg_magnitude
        FROM `bigquery-public-data.noaa_historic_severe_storms.tornadoes`
        GROUP BY year
        ORDER BY year"

# Ejecutar la consulta
resultados <- bq_project_query(project_id, sql)

# Mostrar los resultados
resultados %>%
  bq_table_download() %>%
  head(10)
```

---

### 游댳 **3.3. Microsoft Azure - Machine Learning Distribuido con Azure Synapse Analytics**
**Caso de uso:** Entrenar un modelo de predicci칩n del valor de viviendas usando un dataset de grandes dimensiones.

#### 游댳 **Flujo de trabajo en Azure**
1. Subir el dataset a **Azure Data Lake Storage**.
2. Crear un cl칰ster en **Azure Synapse Analytics**.
3. Conectar R a Synapse utilizando `DBI`.

#### **C칩digo en R usando `DBI` y `sparklyr`**
```r
library(DBI)
library(sparklyr)

# Conectar a Azure Synapse Analytics
sc <- spark_connect(master = "synapse://mi-synapse-workspace")

# Cargar datos desde Azure Data Lake Storage
viviendas <- spark_read_csv(sc, "viviendas", "abfss://datos@mi-datalake.dfs.core.windows.net/viviendas.csv")

# Entrenamiento de un modelo de regresi칩n log칤stica en Spark
modelo <- viviendas %>%
  ml_logistic_regression(response = "precio", features = c("habitaciones", "metros", "ubicacion"))

# Evaluar el modelo
ml_evaluate(modelo)
```

---

### 游댳 **3.4. Databricks - An치lisis de Series Temporales en Apache Spark**
**Caso de uso:** Predecir la demanda de energ칤a el칠ctrica a partir de datos hist칩ricos.

#### 游댳 **Flujo de trabajo en Databricks**
1. Crear un workspace en **Databricks** (AWS, Azure o Google Cloud).
2. Importar el dataset desde un bucket en S3 o Azure Blob Storage.
3. Utilizar Spark ML para el an치lisis de series temporales.

#### **C칩digo en R usando `sparklyr` en Databricks**
```r
library(sparklyr)
library(dplyr)

# Conectar a Spark en Databricks
sc <- spark_connect(method = "databricks")

# Cargar datos de consumo energ칠tico
energia <- spark_read_csv(sc, "energia", "/mnt/datos/energia.csv")

# Modelo ARIMA para series temporales
modelo_arima <- energia %>%
  ml_arima(x = "fecha", y = "consumo")

# Previsi칩n para los pr칩ximos 30 d칤as
forecast <- predict(modelo_arima, newdata = energia)
forecast
```

---

### 游댳 **3.5. Snowflake - An치lisis de Datos en SQL Escalable**
**Caso de uso:** Analizar datos de ventas para encontrar patrones de compra en tiempo real.

#### 游댳 **Flujo de trabajo en Snowflake**
1. Cargar los datos en **Snowflake** desde S3 o Azure Blob Storage.
2. Conectarse a Snowflake usando `DBI`.
3. Ejecutar consultas en SQL directamente desde R.

#### **C칩digo en R usando `DBI` para Snowflake**
```r
library(DBI)

# Conectar a Snowflake
con <- dbConnect(odbc::odbc(), 
                 Driver = "SnowflakeDSIIDriver",
                 Server = "mi-servidor.snowflakecomputing.com",
                 Database = "ventas",
                 UID = "mi_usuario",
                 PWD = "mi_contrase침a")

# Consulta SQL en Snowflake
query <- "SELECT producto, SUM(ventas) as total_ventas
          FROM ventas
          WHERE fecha >= '2024-01-01'
          GROUP BY producto
          ORDER BY total_ventas DESC"

# Obtener los resultados
resultados <- dbGetQuery(con, query)
head(resultados)
```

---

## 游댳 **4. Uso b치sico de Spark con R**
Una vez conectado, puedes cargar y manipular datos directamente en Spark:

```r
# Cargar datos en Spark desde un CSV
datos <- spark_read_csv(sc, name = "datos_spark", path = "ruta/dataset.csv", infer_schema = TRUE)

# Ver las primeras filas
head(datos)

# Transformaciones en Spark usando dplyr
library(dplyr)

datos %>%
  group_by(categoria) %>%
  summarise(promedio = mean(valor)) %>%
  collect()  # 'collect()' trae los resultados a R
```

---

## 游댳 **5. Apagar la sesi칩n de Spark y desinsta**
Cuando termines de usar Spark, cierra la conexi칩n con:

```r
spark_disconnect(sc)
```
Con esto se liberar치n todos los recursos de memoria, disco, etc. asociados a la session de spark.

Para desinstalar spark completamente usa lo siguiente:

```r
spark_uninstall(version = "3.2.0")
```
