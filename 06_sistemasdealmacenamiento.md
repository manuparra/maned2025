# ğŸ“Œ Sistemas de Almacenamiento para Big Data en R y Apache Spark

Sistemas de almacenamiento mÃ¡s populares utilizados para manejar grandes volÃºmenes de datos, centrÃ¡ndonos en la integraciÃ³n con **R** mediante **Apache Spark** (`sparklyr`). Hablaremos de **HDFS**, **Amazon S3** y **Google Cloud Storage**, explicando sus modelos de distribuciÃ³n, ventajas y ejemplos reales de acceso a datos.

---

## ğŸ”¹ **1. Hadoop Distributed File System (HDFS)**

### ğŸ“‚ **Â¿QuÃ© es HDFS?**
**HDFS** es un sistema de archivos distribuido diseÃ±ado para almacenar grandes conjuntos de datos, distribuidos en varios nodos de un clÃºster. Se basa en la replicaciÃ³n de bloques para garantizar tolerancia a fallos y acceso paralelo eficiente.

### ğŸ”¸ **Modelo de DistribuciÃ³n**
- Los datos se dividen en bloques (generalmente de 128 MB).
- Los bloques se replican en varios nodos (por defecto 3 copias).
- El acceso a datos es paralelo y local a los nodos que los almacenan, mejorando el rendimiento.

### ğŸ”¸ **Acceso a HDFS desde R con Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "yarn")

# Leer datos desde HDFS
datos <- spark_read_csv(sc, name = "datos_hdfs", path = "hdfs://cluster/user/data/sample_data.csv")

# Ejemplo real: dataset pÃºblico de la ciudad de Nueva York (Taxi trips)
# Ruta de ejemplo real (pÃºblica en muchos clÃºsteres):
ruta_hdfs <- "hdfs://cluster/public/nyc_taxi/yellow_tripdata_2023-01.csv"
taxi_nyc <- spark_read_csv(sc, name = "taxi_nyc", path = ruta_hdfs, infer_schema = TRUE)

head(taxi_nyc)
```

### ğŸ”¸ **Ventajas de HDFS**
âœ… Alto rendimiento para grandes volÃºmenes de datos en paralelo.  
âœ… Alta tolerancia a fallos mediante replicaciÃ³n.  
âœ… Compatible nativamente con ecosistemas Hadoop/Spark.

---

## ğŸ”¹ **2. Amazon S3 (Simple Storage Service)**

### ğŸ“‚ **Â¿QuÃ© es Amazon S3?**
Amazon S3 es un sistema de almacenamiento basado en objetos, ampliamente utilizado para almacenar datos en la nube, accesible globalmente y escalable sin lÃ­mites prÃ¡cticos.

### ğŸ”¸ **Modelo de DistribuciÃ³n**
- Almacenamiento basado en objetos distribuidos globalmente.
- Escalabilidad automÃ¡tica y acceso paralelo desde mÃºltiples clÃºsteres.
- IntegraciÃ³n directa con Spark y otras herramientas analÃ­ticas.

### ğŸ”¸ **Acceso a S3 desde R con Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

# Configurar acceso con credenciales AWS (se asume configuraciÃ³n previa)
# Sys.setenv(AWS_ACCESS_KEY_ID="tu_key", AWS_SECRET_ACCESS_KEY="tu_secret")

# Leer datos directamente desde Amazon S3
datos_s3 <- spark_read_parquet(sc, name = "datos_s3",
                               path = "s3a://mi-bucket/datos.parquet")

# Ejemplo real: dataset pÃºblico de Landsat en AWS
ruta_s3_landsat <- "s3a://landsat-pds/c1/L8/001/003/LC08_L1TP_001003_20170428_20170510_01_T1/*MTL.txt"
landsat <- spark_read_text(sc, name = "landsat_metadata", path = ruta_s3_landsat)

head(landsat)
```

### ğŸ”¸ **Ventajas de Amazon S3**
âœ… Alta escalabilidad sin mantenimiento del sistema de almacenamiento.  
âœ… IntegraciÃ³n nativa con AWS EMR y Spark.  
âœ… Pago segÃºn uso, con almacenamiento altamente duradero y disponible.

---

## ğŸ”¹ **3. Google Cloud Storage (GCS)**

### ğŸ“‚ **Â¿QuÃ© es Google Cloud Storage?**
Google Cloud Storage es un servicio de almacenamiento de objetos escalable y seguro en la nube de Google, diseÃ±ado para grandes volÃºmenes de datos.

### ğŸ”¸ **Modelo de DistribuciÃ³n**
- Objetos almacenados en "buckets" distribuidos geogrÃ¡ficamente.
- Alta disponibilidad y rendimiento mediante almacenamiento distribuido.
- Acceso eficiente mediante Spark y BigQuery.

### ğŸ”¸ **Acceso a Google Cloud Storage desde R con Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

# AutenticaciÃ³n previa con Google Cloud SDK o mediante archivo JSON de credenciales
# Sys.setenv(GOOGLE_APPLICATION_CREDENTIALS="ruta/credenciales.json")

# Leer datos CSV desde GCS
datos_gcs <- spark_read_csv(sc, name = "datos_gcs",
                            path = "gs://mi-bucket/data/dataset.csv")

# Ejemplo real: Dataset pÃºblico de COVID-19 en GCS
ruta_covid_gcs <- "gs://covid19-open-data/v2/latest/epidemiology.csv"
covid_data <- spark_read_csv(sc, name = "covid_data", path = ruta_covid_gcs, infer_schema = TRUE)

head(covid_data)
```

### ğŸ”¸ **Ventajas de Google Cloud Storage**
âœ… FÃ¡cil integraciÃ³n con Google BigQuery y servicios analÃ­ticos.  
âœ… Muy alto rendimiento para consultas de grandes datasets desde Spark.  
âœ… Seguridad avanzada y excelente gestiÃ³n del ciclo de vida de los datos.

---

## ğŸ”¹ **4. Comparativa de Sistemas de Almacenamiento**

| CaracterÃ­stica        | HDFS                          | Amazon S3                    | Google Cloud Storage   |
|-----------------------|-------------------------------|------------------------------|------------------------|
| **Tipo de Almacenamiento**  | Sistema distribuido en bloques | Basado en objetos            | Basado en objetos      |
| **Modelo de distribuciÃ³n**  | ReplicaciÃ³n en nodos          | DistribuciÃ³n global          | DistribuciÃ³n global    |
| **Velocidad de acceso**     | Alta (localidad en nodos)     | Alta (acceso paralelo global)| Alta (acceso paralelo global) |
| **IntegraciÃ³n con Spark**   | Nativa                        | Muy buena                    | Muy buena              |
| **Costos**                  | Requiere mantenimiento local   | Pago por uso (sin gestiÃ³n)   | Pago por uso (sin gestiÃ³n)  |
| **Ejemplos de uso real**    | Datos empresariales grandes, logs | Datos pÃºblicos (NASA, Landsat)| Datos pÃºblicos (COVID-19, NOAA)|

---

## ğŸ”¹ **5. Modelo de DistribuciÃ³n Local en Nodos**
- **Spark** prioriza el acceso paralelo aprovechando el principio de "data locality": los cÃ¡lculos se realizan lo mÃ¡s cerca posible del nodo donde residen fÃ­sicamente los datos.
- Los datos almacenados en sistemas distribuidos (como HDFS) permiten que Spark ejecute operaciones de forma eficiente, minimizando el movimiento de datos a travÃ©s de la red.

**Ejemplo grÃ¡fico simplificado**:
```
Nodo 1      Nodo 2      Nodo 3
[datos] --> [datos] --> [datos]
  â”‚           â”‚           â”‚
[Spark]     [Spark]     [Spark]
```

Cada nodo procesa simultÃ¡neamente los datos locales, aumentando la eficiencia del acceso paralelo.

---

AquÃ­ tienes el documento ampliado con una secciÃ³n adicional sobre **tipos de ficheros mÃ¡s utilizados en Big Data** y cÃ³mo trabajar con ellos desde **R y Apache Spark**.

---

# ğŸ“Œ **Tipos de Ficheros mÃ¡s usados en Big Data con R y Spark**

En Big Data, la elecciÃ³n del formato del fichero es fundamental para optimizar el almacenamiento, acceso y procesamiento eficiente de grandes conjuntos de datos. A continuaciÃ³n, revisamos los formatos mÃ¡s comunes, sus caracterÃ­sticas, ventajas y ejemplos de uso con R y Spark.

---

## ğŸ”¹ **1. CSV (Comma Separated Values)**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato basado en texto plano.
- Alta compatibilidad y fÃ¡cil manejo.
- Poco eficiente para grandes volÃºmenes de datos (no comprimido, sin indexado).

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

datos_csv <- spark_read_csv(sc, name = "datos_csv", path = "hdfs://ruta/datos.csv")

# Ejemplo pÃºblico: NYC Taxi Dataset (CSV)
ruta_nyc <- "s3a://nyc-tlc/trip data/yellow_tripdata_2023-01.csv"
nyc_taxi <- spark_read_csv(sc, "nyc_taxi", ruta_nyc, infer_schema = TRUE)

head(nyc_taxi)
```

---

## ğŸ”¹ **2. Parquet**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato binario columnar altamente eficiente.
- CompresiÃ³n efectiva y soporte para particionado.
- Ideal para anÃ¡lisis rÃ¡pidos sobre columnas especÃ­ficas.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

datos_parquet <- spark_read_parquet(sc, name = "datos_parquet", path = "s3a://mi-bucket/datos.parquet")

# Ejemplo pÃºblico: Amazon Reviews (Parquet)
ruta_reviews <- "s3a://amazon-reviews-pds/parquet/product_category=Electronics/*.parquet"
reviews <- spark_read_parquet(sc, "amazon_reviews", ruta_reviews)

head(reviews)
```

---

## ğŸ”¹ **3. ORC (Optimized Row Columnar)**

### ğŸ“‚ **CaracterÃ­sticas**
- Similar a Parquet (almacenamiento columnar comprimido).
- Mejor rendimiento en consultas complejas de Hive.
- Muy usado en ecosistemas Hadoop.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "yarn")

datos_orc <- spark_read_orc(sc, name = "datos_orc", path = "hdfs://ruta/datos.orc")

# Ejemplo real (datos de ventas almacenados en Hadoop)
ruta_ventas <- "hdfs://cluster/user/data/ventas.orc"
ventas <- spark_read_orc(sc, "ventas", ruta_ventas)

head(ventas)
```

---

## ğŸ”¹ **4. Avro**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato basado en filas con esquema definido.
- Muy eficiente en sistemas de transmisiÃ³n (Kafka).
- Compatible con esquemas evolutivos.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

# Spark nativamente puede necesitar paquetes adicionales para leer Avro
datos_avro <- spark_read_avro(sc, name = "datos_avro", path = "s3a://ruta/datos.avro")

# Ejemplo pÃºblico: datos climÃ¡ticos en formato Avro
ruta_clima <- "s3a://noaa-avro-pds/datos-clima/*.avro"
clima <- spark_read_avro(sc, "clima", ruta_clima)

head(clima)
```

---

## ğŸ”¹ **5. JSON**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato texto, estructurado, ligero y fÃ¡cil de leer.
- Menos eficiente en Big Data, aunque comÃºn en datos semi-estructurados.

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local")

datos_json <- spark_read_json(sc, name = "datos_json", path = "gs://bucket/datos.json")

# Ejemplo real: datos abiertos de Twitter (JSON)
ruta_twitter <- "gs://public-datasets/twitter_sample/*.json"
tweets <- spark_read_json(sc, "tweets", ruta_twitter)

head(tweets)
```

---

## ğŸ”¹ **6. Feather (Apache Arrow)**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato ligero diseÃ±ado para intercambio rÃ¡pido entre R y Python.
- Alto rendimiento para anÃ¡lisis interactivos.

### ğŸ“Œ **Ejemplo en R**
```r
library(arrow)
library(dplyr)

datos_feather <- read_feather("datos.feather")

head(datos_feather)
```

(No disponible directamente en Spark, pero Ãºtil en entornos hÃ­bridos R/Python)

---

## ğŸ”¹ **7. Delta Lake**

### ğŸ“‚ **CaracterÃ­sticas**
- Formato basado en Parquet con transacciones ACID.
- Soporte para versionado de datos.
- Integrado profundamente con Spark (Databricks).

### ğŸ“Œ **Ejemplo en R/Spark**
```r
library(sparklyr)
sc <- spark_connect(master = "local", packages = "io.delta:delta-core_2.12:2.4.0")

datos_delta <- spark_read_delta(sc, name = "datos_delta", path = "s3a://bucket/delta-table/")

head(datos_delta)
```

---

## ğŸ”¹ **Comparativa de Formatos para Big Data**

| Formato | CompresiÃ³n | Rendimiento Lectura | Uso tÃ­pico | IntegraciÃ³n Spark |
|---------|------------|---------------------|------------|-------------------|
| CSV     | âŒ Baja    | âš ï¸ Medio-Bajo        | Datos abiertos, fÃ¡cil uso | âœ… Completa |
| Parquet | âœ… Alta    | ğŸš€ Alto (columnar)   | AnalÃ­tica eficiente | âœ… Completa |
| ORC     | âœ… Alta    | ğŸš€ Alto (columnar)   | Hadoop/Hive  | âœ… Completa |
| Avro    | âœ… Alta    | âš ï¸ Medio-Alto        | Streaming, intercambio estructurado | âœ… Completa |
| JSON    | âš ï¸ Medio   | âš ï¸ Medio-Bajo        | Semi-estructurados, APIs | âœ… Completa |
| Feather | âš ï¸ Medio-Alto | ğŸš€ Muy Alto       | AnÃ¡lisis rÃ¡pido (R-Python) | âŒ Indirecta |
| Delta Lake | âœ… Alta | ğŸš€ Muy Alto (versionado)| Data lakes avanzados | âœ… Completa |

