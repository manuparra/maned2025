# üîµ **Tema 6.5.6 ‚Äì Optimizaci√≥n del Rendimiento en Big Data**

La optimizaci√≥n del rendimiento es una fase cr√≠tica en proyectos de **Big Data**, especialmente cuando trabajamos con grandes vol√∫menes de datos en entornos distribuidos como **Apache Spark**. Una ejecuci√≥n ineficiente puede consumir muchos recursos, aumentar los tiempos de c√≥mputo e impactar negativamente en la escalabilidad del sistema.

En esta secci√≥n aprender√°s a:

‚úÖ Comprender los conceptos clave para optimizar el rendimiento en Spark  
‚úÖ Utilizar t√©cnicas avanzadas como **Data Locality**, **Particionado**, **Cach√©/Persistencia** y **Optimizaci√≥n de Memoria**  
‚úÖ Elegir el mejor formato de almacenamiento para mejorar la velocidad y reducir el uso de recursos  
‚úÖ Aplicar estas t√©cnicas usando el dataset real de **NYC Taxi Trips**  
‚úÖ Implementar ejemplos pr√°cticos con mejoras cuantificables en tiempos de ejecuci√≥n  

---

# üîπ **1. Conceptos clave para optimizaci√≥n en Spark**

Apache Spark es un entorno distribuido que divide el trabajo en **tareas** (tasks) ejecutadas en **nodos** del cl√∫ster. Optimizar el rendimiento requiere minimizar el tiempo que Spark emplea en transferir datos y maximizar el aprovechamiento de la memoria y CPU.

---

## üìå **1.1. Data Locality (Localidad de datos)**
El concepto de **localidad de datos** se refiere a ejecutar las tareas lo m√°s cerca posible de los datos que se est√°n procesando. Spark intenta ubicar las tareas en los mismos nodos donde se encuentran los datos, minimizando la transferencia a trav√©s de la red.

### üîé **C√≥mo optimizar la localidad de datos**
1. Usar formatos que favorezcan el almacenamiento distribuido como **Parquet** o **ORC**.  
2. Utilizar sistemas de archivos distribuidos como **HDFS** o **Amazon S3**.  
3. Configurar correctamente el particionado para garantizar que los datos est√©n repartidos de forma equilibrada.

**Ejemplo:** Cargar el dataset de taxis en formato Parquet, que optimiza la localidad de datos.

```r
# Cargar datos en formato Parquet (favorece la localidad de datos)
taxi_nyc <- spark_read_parquet(sc,
                               name = "taxi_nyc",
                               path = "hdfs://ruta/dataset.parquet")
```

---

## üìå **1.2. Particionado y Replicaci√≥n de Datos**
El **particionado** es el proceso de dividir grandes datasets en bloques m√°s peque√±os que se pueden procesar en paralelo. Esto es clave para mejorar el rendimiento en Spark.

### üîé **C√≥mo optimizar el particionado**
- Un particionado demasiado peque√±o genera demasiadas tareas peque√±as que ralentizan el rendimiento.  
- Un particionado demasiado grande sobrecarga la memoria de cada nodo.  
- El n√∫mero ideal de particiones se calcula generalmente como **4 x n√∫mero de CPUs disponibles**.

**Configuraci√≥n del n√∫mero de particiones √≥ptimo**
```r
# Repartir el dataset en 100 particiones para optimizar el rendimiento
taxi_nyc <- taxi_nyc %>%
  sdf_repartition(partitions = 100)
```

---

## üìå **1.3. Formatos de Almacenamiento Eficientes**
Elegir el formato adecuado de almacenamiento es clave en proyectos Big Data.  

| **Formato** | **Ventajas** | **Uso recomendado** |
|---------------|------------------|-------------------------|
| **Parquet** | Eficiente en consultas anal√≠ticas; Soporta compresi√≥n y columnar | Consultas frecuentes sobre grandes vol√∫menes de datos |
| **ORC** | Similar a Parquet; Excelente en entornos Hadoop | Procesamiento intensivo en Spark y Hive |
| **Delta Lake** | Extiende Parquet con transacciones ACID y versionado de datos | Pipeline de datos confiable en producci√≥n |

---

### üîé **Ejemplo: Conversi√≥n de CSV a Parquet**
```r
# Cargar el dataset en formato CSV
taxi_nyc_csv <- spark_read_csv(sc,
                               name = "taxi_nyc_csv",
                               path = "hdfs://ruta/nyc_taxi.csv")

# Guardar en formato Parquet (mucho m√°s eficiente)
spark_write_parquet(taxi_nyc_csv, path = "hdfs://ruta/nyc_taxi.parquet")
```

---

## üìå **1.4. Cach√© y Persistencia en Spark**
El **cach√©** y la **persistencia** permiten almacenar temporalmente los datos en memoria para acelerar el acceso en c√°lculos repetidos.

### üîé **Diferencias entre Cache y Persistencia**
| M√©todo | Caracter√≠sticas |
|---------|-----------------|
| **Cache()** | Almacena temporalmente los datos en memoria para accesos r√°pidos; solo es v√°lido dentro de la sesi√≥n actual. |
| **Persist()** | Similar a `cache()` pero permite elegir niveles de almacenamiento (RAM, disco o ambos). |

### üîé **Cu√°ndo usar cada uno**
- Utiliza **`cache()`** cuando vas a reutilizar un dataset en m√∫ltiples pasos dentro de una misma sesi√≥n.  
- Utiliza **`persist()`** cuando el dataset es grande y quieres control sobre c√≥mo se almacena.

---

### üîé **Ejemplo: Uso de `cache()` para optimizar consultas repetidas**
```r
# Cargar datos en memoria para reutilizaci√≥n r√°pida
taxi_nyc <- taxi_nyc %>% cache()

# Consulta r√°pida usando los datos en cach√©
taxi_nyc %>%
  group_by(DATE(pickup_datetime)) %>%
  summarise(total_ingresos = sum(fare_amount)) %>%
  collect()
```

---

### üîé **Ejemplo: Uso de `persist()` en Spark**
```r
# Persistir el dataset en memoria + disco (recomendado para conjuntos muy grandes)
taxi_nyc %>% persist("MEMORY_AND_DISK")
```

---

## üìå **1.5. Optimizaci√≥n del Tama√±o de Particiones**
El tama√±o de cada partici√≥n es un factor cr√≠tico en el rendimiento de Spark. Si las particiones son demasiado grandes, algunas tareas ocupar√°n m√°s tiempo que otras, reduciendo la eficiencia del cl√∫ster.

### üîé **Configuraci√≥n √≥ptima de particiones en Spark**
```r
# Optimizar n√∫mero de particiones en base a la cantidad de nodos disponibles
taxi_nyc <- sdf_repartition(taxi_nyc, partitions = 200)
```

---

# üîπ **2. Ejemplos Pr√°cticos de Optimizaci√≥n en Spark**

## ‚û§ **Ejemplo 1 ‚Äì Comparaci√≥n de tiempos de consulta con y sin optimizaci√≥n**
Este ejemplo eval√∫a el impacto del uso de `cache()` y formato **Parquet** en tiempos de consulta.

### **Paso 1:** Carga inicial sin optimizaci√≥n
```r
system.time({
  taxi_nyc %>%
    group_by(DATE(pickup_datetime)) %>%
    summarise(total_ingresos = sum(fare_amount)) %>%
    collect()
})
```

### **Paso 2:** Carga optimizada con `cache()` y Parquet
```r
# Almacenar en Parquet (optimizado)
spark_write_parquet(taxi_nyc, path = "hdfs://ruta/nyc_taxi.parquet")

# Cargar el dataset Parquet y usar cache
taxi_nyc_opt <- spark_read_parquet(sc, name = "taxi_nyc_opt", path = "hdfs://ruta/nyc_taxi.parquet") %>%
  cache()

# Comparar tiempos
system.time({
  taxi_nyc_opt %>%
    group_by(DATE(pickup_datetime)) %>%
    summarise(total_ingresos = sum(fare_amount)) %>%
    collect()
})
```

---

## üîπ **3. Ejercicios Pr√°cticos**

### üü© **Ejercicio 1 ‚Äì Optimizaci√≥n con Parquet y Cache**
**Objetivo:**  
- Cargar el dataset de taxis en formato **CSV**.  
- Convertir el dataset a **Parquet**.  
- Utilizar `cache()` para optimizar la ejecuci√≥n de una consulta que calcule la distancia media por d√≠a.  

**C√≥digo orientativo:**
```r

```

---

### üü© **Ejercicio 2 ‚Äì Optimizaci√≥n del Particionado**
**Objetivo:**  
- Cargar el dataset de taxis en formato **Parquet**.  
- Repartir los datos en **100 particiones** para optimizar el rendimiento.  
- Calcular la tarifa promedio por d√≠a.  

**C√≥digo orientativo:**
```r

```

