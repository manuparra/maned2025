# ğŸ”¥ **SecciÃ³n 6.7: Machine Learning con Big Data y `sparklyr`**


## ğŸ“š **6.7.1. IntroducciÃ³n al Machine Learning en Spark con `sparklyr`**
âœ… Conceptos clave de Machine Learning en Spark.  
âœ… Arquitectura de **MLlib** (LibrerÃ­a de Machine Learning de Spark).  
âœ… Ventajas de usar Spark para el entrenamiento de modelos sobre grandes volÃºmenes de datos.  
âœ… ComparaciÃ³n entre el flujo de trabajo tradicional en R y el flujo de trabajo en Spark.  
âœ… IntegraciÃ³n de Spark con librerÃ­as avanzadas como **`mlflow`** para el seguimiento de experimentos.

La combinaciÃ³n de **`sparklyr`** y **Apache Spark** proporciona un entorno escalable y eficiente para realizar **Machine Learning (ML)** sobre grandes volÃºmenes de datos. En esta secciÃ³n, exploraremos conceptos clave, la arquitectura de Spark para ML, sus ventajas y cÃ³mo implementar un flujo de trabajo eficiente.

---

## ğŸ”¹ **Conceptos clave de Machine Learning en Spark**

### ğŸš€ **Â¿Por quÃ© usar Spark para Machine Learning?**
Cuando trabajamos con grandes volÃºmenes de datos, los algoritmos tradicionales de Machine Learning en R (como `lm()`, `randomForest()` o `xgboost()`) pueden no ser viables debido a:

â— **Limitaciones de memoria:** Al cargar grandes conjuntos de datos en la memoria local.  
â— **Baja escalabilidad:** Los algoritmos tradicionales no estÃ¡n diseÃ±ados para escalar horizontalmente en mÃºltiples nodos.  

ğŸ”¹ **Apache Spark**, mediante su componente **MLlib**, permite entrenar modelos predictivos de forma distribuida, aprovechando la arquitectura paralela de Spark.

---

## ğŸ”¹ **Arquitectura de `MLlib` en Spark**
**MLlib** es la librerÃ­a nativa de Machine Learning en Apache Spark. EstÃ¡ diseÃ±ada para trabajar con grandes volÃºmenes de datos de forma distribuida.

ğŸ”¹ **Componentes Clave de `MLlib`:**  
âœ… **`ml_pipeline()`** â†’ Permite construir flujos de trabajo completos de Machine Learning.  
âœ… **`ml_regression()`** â†’ Modelos de regresiÃ³n (lineal, logÃ­stica, etc.).  
âœ… **`ml_classification()`** â†’ Modelos de clasificaciÃ³n como Ãrboles de DecisiÃ³n o Random Forest.  
âœ… **`ml_clustering()`** â†’ Algoritmos de clustering como **K-Means**.  
âœ… **`ml_recommendation()`** â†’ Sistemas de recomendaciÃ³n basados en el algoritmo **ALS** (Alternating Least Squares).  
âœ… **`ml_evaluate()`** â†’ Herramienta para evaluar el rendimiento de los modelos.

---

## ğŸ”¹ **Ventajas de Usar Spark para Machine Learning**

âœ… **Escalabilidad:** Spark puede manejar grandes volÃºmenes de datos gracias a su arquitectura distribuida.  
âœ… **Eficiencia:** Procesamiento optimizado al mantener los datos en memoria (**in-memory computing**).  
âœ… **Flexibilidad:** Compatible con mÃºltiples lenguajes de programaciÃ³n, incluido R.  
âœ… **IntegraciÃ³n Nativa:** Se conecta fÃ¡cilmente con entornos de almacenamiento distribuido como **HDFS**, **S3**, y **Azure Blob Storage**.  
âœ… **AutomatizaciÃ³n con Pipelines:** Permite encadenar mÃºltiples etapas del flujo de trabajo de ML en una Ãºnica estructura organizada.  

---

## ğŸ”¹ **ComparaciÃ³n entre el Flujo de Trabajo Tradicional en R vs Spark**

| Aspecto                 | Flujo Tradicional en R          | Flujo en Spark con `sparklyr` |
|--------------------------|----------------------------------|------------------------------|
| **Carga de Datos**          | `read.csv()` o `read_parquet()` (lectura en memoria local). | `spark_read_csv()` o `spark_read_parquet()` (lectura distribuida). |
| **Limpieza de Datos**        | Operaciones con `dplyr` o `data.table`. | Operaciones con `dplyr` sobre Spark DataFrames. |
| **Entrenamiento del Modelo** | Funciones tradicionales (`lm()`, `randomForest()`, etc.). | Algoritmos escalables de `ml_*()` en Spark. |
| **EvaluaciÃ³n del Modelo**    | `caret::confusionMatrix()` o `yardstick`. | `ml_evaluate()` para evaluar modelos directamente en Spark. |
| **Despliegue del Modelo**    | Guardar el modelo como `.RDS` o `.h5`. | `ml_save()` para almacenar el modelo directamente en el clÃºster. |

---

## ğŸ”¹ **IntegraciÃ³n de Spark con `mlflow` para el Seguimiento de Experimentos**
**MLflow** es una potente herramienta que facilita el seguimiento, la comparaciÃ³n y la organizaciÃ³n de experimentos de Machine Learning en entornos distribuidos.

ğŸ”¹ **CaracterÃ­sticas principales de `mlflow`:**  
âœ… Permite registrar mÃ©tricas, parÃ¡metros y artefactos del modelo.  
âœ… Facilita la comparaciÃ³n de diferentes versiones de un mismo modelo.  
âœ… Es compatible con Spark, permitiendo la gestiÃ³n de experimentos de ML directamente en `sparklyr`.

---

## ğŸ”¹ **Ejemplo PrÃ¡ctico: IntroducciÃ³n al Machine Learning con el Dataset de NYC Yellow Cab**

En este ejemplo, implementaremos un flujo bÃ¡sico de Machine Learning utilizando el dataset **NYC Yellow Cab** en formato Parquet.

---

### ğŸ“‹ **Paso 1: Cargar el Dataset en Spark**
El dataset contiene informaciÃ³n sobre viajes de taxis en Nueva York, incluyendo columnas como:

- **`pickup_datetime`** â†’ Fecha y hora del inicio del viaje.  
- **`trip_distance`** â†’ Distancia del viaje en millas.  
- **`fare_amount`** â†’ Tarifa total pagada por el viaje.  
- **`passenger_count`** â†’ NÃºmero de pasajeros en el vehÃ­culo.  

**CÃ³digo en R para cargar el dataset**
```r
library(sparklyr)
library(dplyr)

# Conectar a Spark
sc <- spark_connect(master = "local[*]")

# Cargar el dataset desde Parquet
taxis_nyc <- spark_read_parquet(
  sc, 
  name = "taxis_nyc", 
  path = "yellow_tripdata_2023-01.parquet"
)

# Visualizar el esquema del dataset
sdf_schema(taxis_nyc)
```

---

### ğŸ“‹ **Paso 2: ExploraciÃ³n y Limpieza de Datos**
- Filtraremos viajes invÃ¡lidos (por ejemplo, con distancia negativa o tarifas cero).  
- Removeremos valores atÃ­picos en las columnas clave.  

**CÃ³digo en R**
```r
# Filtrado bÃ¡sico para limpiar datos incorrectos
taxis_nyc <- taxis_nyc %>%
  filter(trip_distance > 0, fare_amount > 0)

# Verificar la cantidad de registros tras limpieza
taxis_nyc %>% sdf_nrow()
```

---

### ğŸ“‹ **Paso 3: DivisiÃ³n del Dataset en Entrenamiento y Prueba**
Usaremos la funciÃ³n **`sdf_partition()`** para dividir el dataset.

**CÃ³digo en R**
```r
# DivisiÃ³n del dataset en entrenamiento (80%) y prueba (20%)
particiones <- taxis_nyc %>%
  sdf_partition(training = 0.8, testing = 0.2, seed = 1234)

# AsignaciÃ³n de subconjuntos
taxis_train <- particiones$training
taxis_test <- particiones$testing
```

---

### ğŸ“‹ **Paso 4: Entrenamiento de un Modelo de RegresiÃ³n Lineal**
Vamos a predecir el valor de la tarifa (`fare_amount`) en funciÃ³n de la distancia (`trip_distance`) y el nÃºmero de pasajeros (`passenger_count`).

**CÃ³digo en R**
```r
modelo_regresion <- taxis_train %>%
  ml_linear_regression(fare_amount ~ trip_distance + passenger_count)

# Evaluar el modelo en el conjunto de prueba
predicciones <- ml_predict(modelo_regresion, taxis_test)

# Visualizar predicciones
predicciones %>%
  select(fare_amount, prediction) %>%
  collect() %>%
  head(10)
```

---

### ğŸ“‹ **Paso 5: EvaluaciÃ³n del Modelo**
Evaluaremos el rendimiento utilizando el **RMSE** (Error CuadrÃ¡tico Medio) y el **RÂ²**.

**CÃ³digo en R**
```r
# EvaluaciÃ³n del modelo
ml_evaluate(modelo_regresion, taxis_test)
```

---

### ğŸ“‹ **Paso 6: Guardar el Modelo para su Uso en ProducciÃ³n**
El modelo se puede guardar directamente en el sistema de archivos o en un entorno cloud.

**CÃ³digo en R**
```r
ml_save(modelo_regresion, "modelo_regresion_spark")
```

---

## ğŸ“š **6.7.2. PreparaciÃ³n de los Datos para Machine Learning en Spark**
âœ… IngenierÃ­a de caracterÃ­sticas (Feature Engineering) en Spark.  

La **IngenierÃ­a de CaracterÃ­sticas** (Feature Engineering) es una fase crÃ­tica en el flujo de trabajo de **Machine Learning**, ya que influye directamente en el rendimiento del modelo. Consiste en crear, transformar o seleccionar variables (features) que mejor representen la informaciÃ³n contenida en los datos.

En entornos de **Big Data**, como el dataset de **NYC Yellow Cab**, **Spark** permite realizar estas transformaciones de forma escalable y eficiente gracias a la integraciÃ³n de **`sparklyr`**.

---

## ğŸ”¹ **Importancia de la IngenierÃ­a de CaracterÃ­sticas**
âœ… Mejora la capacidad predictiva del modelo.  
âœ… Reduce la dimensionalidad eliminando variables irrelevantes.  
âœ… Transforma datos crudos en representaciones mÃ¡s significativas.  
âœ… Facilita que los algoritmos de Machine Learning converjan mÃ¡s rÃ¡pido.  

---

## ğŸ”¹ **TÃ©cnicas de IngenierÃ­a de CaracterÃ­sticas en Spark**

A continuaciÃ³n, se detallan tÃ©cnicas clave de Feature Engineering implementadas en Spark utilizando el dataset de **NYC Yellow Cab** en formato **Parquet**.

---

## ğŸ“Œ **1. GeneraciÃ³n de Nuevas Variables Temporales**
El dataset de **NYC Yellow Cab** incluye una columna **`pickup_datetime`**, que puede explotarse para generar mÃºltiples caracterÃ­sticas Ãºtiles.

### ğŸ” **Ejemplo: Extraer el dÃ­a de la semana y la hora del dÃ­a**
```r
library(sparklyr)
library(dplyr)

# Conectar a Spark
sc <- spark_connect(master = "local[*]")

# Cargar el dataset
taxis_nyc <- spark_read_parquet(
  sc,
  name = "taxis_nyc",
  path = "yellow_tripdata_2023-01.parquet"
)

# Generar nuevas variables temporales
taxis_nyc <- taxis_nyc %>%
  mutate(
    pickup_hour = hour(pickup_datetime),
    pickup_day = weekday(pickup_datetime)
  )

# VisualizaciÃ³n de datos transformados
taxis_nyc %>%
  select(pickup_datetime, pickup_hour, pickup_day) %>%
  head(10) %>%
  collect()
```

âœ… **Â¿Por quÃ© es Ãºtil?**  
- Las horas pico (rush hours) suelen influir en la duraciÃ³n del viaje y el costo.  
- Los dÃ­as festivos o fines de semana pueden impactar el comportamiento de las tarifas.

---

## ğŸ“Œ **2. TransformaciÃ³n de Variables CategÃ³ricas con `ft_string_indexer()`**
Las variables categÃ³ricas no se pueden usar directamente en algoritmos como regresiÃ³n lineal o Ã¡rboles de decisiÃ³n. Spark permite convertirlas en variables numÃ©ricas mediante **indexaciÃ³n** o **one-hot encoding**.

### ğŸ” **Ejemplo: Indexar la columna `payment_type`**
```r
# Transformar la columna 'payment_type' a Ã­ndices numÃ©ricos
taxis_nyc <- taxis_nyc %>%
  ft_string_indexer("payment_type", "payment_type_indexed")

# VisualizaciÃ³n de la columna transformada
taxis_nyc %>%
  select(payment_type, payment_type_indexed) %>%
  distinct() %>%
  collect()
```

âœ… **Â¿Por quÃ© es Ãºtil?**  
- Permite que los algoritmos procesen variables categÃ³ricas.  
- Es especialmente valioso en Spark para evitar que la codificaciÃ³n ralentice el entrenamiento del modelo.

---

## ğŸ“Œ **3. Escalado y NormalizaciÃ³n de Variables con `ft_standard_scaler()`**
El escalado es importante para que las variables tengan una escala comparable, especialmente en algoritmos que se basan en distancias (como **KNN** o **K-Means**).

### ğŸ” **Ejemplo: Escalar la columna `trip_distance`**
```r
# Escalado de la distancia del viaje
taxis_nyc <- taxis_nyc %>%
  ft_vector_assembler(input_cols = "trip_distance", output_col = "trip_distance_vec") %>%
  ft_standard_scaler(input_col = "trip_distance_vec", output_col = "trip_distance_scaled")

# VisualizaciÃ³n de la distancia escalada
taxis_nyc %>%
  select(trip_distance, trip_distance_scaled) %>%
  head(10) %>%
  collect()
```

âœ… **Â¿Por quÃ© es Ãºtil?**  
- Evita que variables con escalas grandes dominen el modelo.  
- Es fundamental en algoritmos como **RegresiÃ³n Lineal**, **SVM**, o **K-Means**.

---

## ğŸ“Œ **4. Tratamiento de Valores AtÃ­picos (Outliers)**
Los valores extremos pueden afectar negativamente el rendimiento del modelo. En Spark, se pueden manejar de distintas maneras:

âœ… **Filtrado de valores atÃ­picos**  
âœ… **WinsorizaciÃ³n** (ajustar los valores extremos a un umbral)  
âœ… **TransformaciÃ³n logarÃ­tmica** para reducir el impacto de los outliers.

### ğŸ” **Ejemplo: Filtrado de tarifas extremadamente altas**
```r
# Filtrar tarifas superiores al percentil 99
percentil_99 <- taxis_nyc %>%
  summarise(limite_superior = percentile_approx(fare_amount, 0.99)) %>%
  collect() %>%
  pull(limite_superior)

# Filtrar outliers
taxis_nyc <- taxis_nyc %>%
  filter(fare_amount < percentil_99)

# VisualizaciÃ³n del nuevo rango de tarifas
taxis_nyc %>%
  summarise(min_fare = min(fare_amount), max_fare = max(fare_amount)) %>%
  collect()
```

âœ… **Â¿Por quÃ© es Ãºtil?**  
- Evita que los modelos se ajusten excesivamente a datos extremos.  
- Mejora la precisiÃ³n de algoritmos sensibles a outliers (como regresiÃ³n lineal).

---

## ğŸ“Œ **5. CombinaciÃ³n de Variables (Feature Interaction)**
Crear nuevas variables combinando las existentes puede mejorar significativamente el rendimiento del modelo.

### ğŸ” **Ejemplo: Crear una nueva variable 'tarifa por milla'**
```r
# Crear una nueva variable: tarifa por milla
taxis_nyc <- taxis_nyc %>%
  mutate(fare_per_mile = fare_amount / trip_distance)

# VisualizaciÃ³n del resultado
taxis_nyc %>%
  select(fare_amount, trip_distance, fare_per_mile) %>%
  head(10) %>%
  collect()
```

âœ… **Â¿Por quÃ© es Ãºtil?**  
- Permite capturar relaciones no lineales en los datos.  
- Facilita la identificaciÃ³n de patrones que no son evidentes en las variables originales.

---

## ğŸ“Œ **6. ImputaciÃ³n de Valores Faltantes**
El manejo de datos faltantes es crucial para que los algoritmos de Machine Learning funcionen correctamente.

### ğŸ” **Ejemplo: Imputar valores faltantes en la columna `passenger_count` usando la mediana**
```r
# Imputar valores faltantes con la mediana
taxis_nyc <- taxis_nyc %>%
  mutate(passenger_count = ifelse(is.na(passenger_count), median(passenger_count, na.rm = TRUE), passenger_count))

# ComprobaciÃ³n de datos sin NA
taxis_nyc %>%
  summarise(total_na = sum(is.na(passenger_count))) %>%
  collect()
```

âœ… **Â¿Por quÃ© es Ãºtil?**  
- Evita que el modelo ignore registros valiosos.  
- Reduce el sesgo introducido por la eliminaciÃ³n directa de filas.

---

## ğŸ“Œ **7. SelecciÃ³n de Variables (Feature Selection)**
Eliminar variables irrelevantes mejora la eficiencia del modelo y reduce el riesgo de sobreajuste (**overfitting**).

### ğŸ” **Ejemplo: Eliminar columnas redundantes**
```r
# Seleccionar solo las columnas relevantes para el modelo
taxis_nyc <- taxis_nyc %>%
  select(fare_amount, trip_distance, passenger_count, pickup_hour)
```

âœ… **Â¿Por quÃ© es Ãºtil?**  
- Mejora el rendimiento del modelo.  
- Facilita la interpretabilidad del modelo.

---

## ğŸ“š **6.7.3. DivisiÃ³n del Dataset en Conjuntos de Entrenamiento y Prueba**
âœ… Uso de `sdf_partition()` para dividir los datos de forma eficiente.  
âœ… Estrategias para manejar datasets desbalanceados.  
âœ… TÃ©cnicas avanzadas para generar subconjuntos representativos en entornos Big Data.  

La **divisiÃ³n de datos** en conjuntos de entrenamiento y prueba es un paso crucial en el flujo de trabajo de Machine Learning. Esta etapa garantiza que el modelo se evalÃºe con datos que no fueron vistos durante el entrenamiento, proporcionando una estimaciÃ³n mÃ¡s precisa de su rendimiento.

Cuando se trabaja con grandes volÃºmenes de datos, como el dataset de **NYC Yellow Cab**, Spark permite manejar esta fase de forma eficiente utilizando herramientas diseÃ±adas para entornos distribuidos.

---

## ğŸ”¹ **Importancia de Dividir los Datos Correctamente**
âœ… Permite evaluar de forma objetiva el rendimiento del modelo.  
âœ… Evita el problema del **sobreajuste** (overfitting).  
âœ… Facilita el ajuste de hiperparÃ¡metros y la selecciÃ³n del mejor modelo.  
âœ… En datasets grandes, una divisiÃ³n eficiente es clave para garantizar el rendimiento.  

---

## ğŸ”¹ **1. Uso de `sdf_partition()` para Dividir los Datos de Forma Eficiente**

La funciÃ³n **`sdf_partition()`** es la herramienta mÃ¡s eficiente en Spark para dividir un dataset en mÃºltiples subconjuntos de forma distribuida.

### ğŸ” **Sintaxis de `sdf_partition()`**
```r
sdf_partition(
  dataset,
  training = 0.8,
  testing = 0.2,
  seed = 1234  # Para reproducibilidad
)
```

ğŸ”¹ **`training = 0.8`** â†’ El 80% de los datos se asignan al conjunto de entrenamiento.  
ğŸ”¹ **`testing = 0.2`** â†’ El 20% restante se asigna al conjunto de prueba.  
ğŸ”¹ **`seed = 1234`** â†’ Permite obtener los mismos resultados en cada ejecuciÃ³n.  

---

### ğŸ“‹ **Ejemplo PrÃ¡ctico: DivisiÃ³n del Dataset de NYC Yellow Cab**

#### Paso 1: Cargar el Dataset
```r
library(sparklyr)
library(dplyr)

# Conectar a Spark
sc <- spark_connect(master = "local[*]")

# Cargar el dataset desde Parquet
taxis_nyc <- spark_read_parquet(
  sc,
  name = "taxis_nyc",
  path = "yellow_tripdata_2023-01.parquet"
)
```

---

#### Paso 2: Dividir los Datos
```r
# DivisiÃ³n del dataset en entrenamiento (80%) y prueba (20%)
particiones <- taxis_nyc %>%
  sdf_partition(
    training = 0.8,
    testing = 0.2,
    seed = 1234
  )

# Asignar subconjuntos
taxis_train <- particiones$training
taxis_test <- particiones$testing

# ComprobaciÃ³n del tamaÃ±o de cada conjunto
cat("Registros en el conjunto de entrenamiento: ", sdf_nrow(taxis_train), "\n")
cat("Registros en el conjunto de prueba: ", sdf_nrow(taxis_test), "\n")
```

âœ… **Ventaja:** `sdf_partition()` realiza la divisiÃ³n de forma distribuida, lo que permite manejar grandes volÃºmenes de datos sin problemas de memoria.

---

## ğŸ”¹ **2. Estrategias para Manejar Datasets Desbalanceados**

En problemas de clasificaciÃ³n, es comÃºn que una clase tenga muchos mÃ¡s registros que otra. Esto puede sesgar el modelo predictivo, reduciendo su capacidad para identificar correctamente las clases minoritarias.

### ğŸ” **TÃ©cnicas para manejar desequilibrios en los datos**
âœ… **Submuestreo (Undersampling):** Reducir el nÃºmero de registros en la clase mayoritaria.  
âœ… **Sobremuestreo (Oversampling):** Aumentar el nÃºmero de registros en la clase minoritaria.  
âœ… **Ajuste de pesos en el modelo:** Asignar mayor peso a las clases minoritarias durante el entrenamiento.  

---

### ğŸ“‹ **Ejemplo: IdentificaciÃ³n de un Dataset Desbalanceado**

#### Paso 1: Comprobar el Balance de Clases
```r
# Comprobar la distribuciÃ³n de la variable 'payment_type'
taxis_nyc %>%
  group_by(payment_type) %>%
  summarise(total = n()) %>%
  collect()
```

Si el resultado muestra que una clase domina claramente (por ejemplo, el 90% de los registros pertenece a una clase), se recomienda aplicar una de las siguientes tÃ©cnicas.

---

### ğŸ“‹ **Ejemplo 1: Submuestreo (Undersampling)**
```r
# Seleccionar aleatoriamente un subconjunto de la clase mayoritaria
min_clase <- taxis_nyc %>%
  filter(payment_type == "Credit card") %>%
  sample_frac(0.5)

# Combinar con la clase minoritaria
taxis_balanceado <- taxis_nyc %>%
  filter(payment_type != "Credit card") %>%
  union(min_clase)
```

---

### ğŸ“‹ **Ejemplo 2: Sobremuestreo (Oversampling)**
```r
# Duplicar registros de la clase minoritaria
minority_class <- taxis_nyc %>%
  filter(payment_type == "Cash")

# Duplicar los registros para equilibrar el dataset
taxis_balanceado <- taxis_nyc %>%
  union(minority_class) %>%
  union(minority_class)
```

âœ… **RecomendaciÃ³n:** El sobre/submuestreo puede aplicarse con `sample_frac()` en Spark, garantizando eficiencia en grandes volÃºmenes de datos.

---

### ğŸ“‹ **Ejemplo 3: Ajuste de Pesos en el Modelo**
En algoritmos como **`ml_logistic_regression()`**, se puede especificar un parÃ¡metro de ponderaciÃ³n (`weight_col`) para dar mayor relevancia a la clase minoritaria.

```r
modelo <- ml_logistic_regression(
  taxis_train,
  fare_amount ~ trip_distance + passenger_count,
  weight_col = "class_weight"
)
```

---

## ğŸ”¹ **3. TÃ©cnicas Avanzadas para Generar Subconjuntos Representativos**

Cuando se manejan grandes volÃºmenes de datos en Spark, dividir el dataset correctamente es clave para garantizar que:

âœ… Los subconjuntos conserven la misma distribuciÃ³n estadÃ­stica que el dataset original.  
âœ… Los datos se distribuyan de forma eficiente para maximizar el rendimiento en el entrenamiento.  

---

### ğŸ“‹ **Ejemplo 1: Muestreo Estratificado (Stratified Sampling)**
El **muestreo estratificado** garantiza que cada subconjunto tenga la misma proporciÃ³n de registros para cada categorÃ­a.

```r
# DivisiÃ³n estratificada por 'payment_type'
particiones <- taxis_nyc %>%
  sdf_partition(
    training = 0.8,
    testing = 0.2,
    stratify_by = "payment_type",
    seed = 1234
  )

taxis_train <- particiones$training
taxis_test <- particiones$testing
```

âœ… **Ventaja:** El muestreo estratificado es ideal para problemas de clasificaciÃ³n con clases desbalanceadas.  

---

### ğŸ“‹ **Ejemplo 2: Muestreo Aleatorio (Random Sampling)**
Si no se requiere que las clases estÃ©n balanceadas, el muestreo aleatorio es una opciÃ³n rÃ¡pida y eficiente.

```r
# Muestreo del 30% de los datos de forma aleatoria
taxis_sample <- taxis_nyc %>%
  sdf_sample(fraction = 0.3, replacement = FALSE)
```

âœ… **Ventaja:** Muy eficiente para datasets extremadamente grandes.

---

### ğŸ“‹ **Ejemplo 3: Muestreo Temporal (Time-Based Sampling)**
En datos temporales, se recomienda dividir el dataset en funciÃ³n de rangos de fechas para que el modelo se entrene con datos histÃ³ricos y se valide con datos recientes.

```r
# Dividir por fecha
taxis_train <- taxis_nyc %>%
  filter(pickup_datetime < "2023-01-20")

taxis_test <- taxis_nyc %>%
  filter(pickup_datetime >= "2023-01-20")
```

âœ… **Ventaja:** Ideal para modelos predictivos basados en series temporales.


---

## ğŸ“š **6.7.4. Modelos de Machine Learning en `sparklyr`**
âœ… Modelos de regresiÃ³n lineal y logÃ­stica en Spark.  
âœ… Modelos de clasificaciÃ³n como **Random Forest**, **Decision Trees**, **SVM**.  
âœ… Modelos de regresiÃ³n para predicciones continuas.  
âœ… Modelos de clustering como **K-Means**

Spark ofrece una potente suite de algoritmos de **Machine Learning** a travÃ©s de su librerÃ­a **MLlib**, que permite realizar entrenamientos escalables, eficientes y distribuidos sobre grandes volÃºmenes de datos. La integraciÃ³n de Spark con R mediante el paquete **`sparklyr`** facilita la implementaciÃ³n de estos modelos de forma sencilla.

En esta secciÃ³n se describen los principales modelos de Machine Learning soportados en Spark, acompaÃ±ados de ejemplos prÃ¡cticos utilizando el dataset de **NYC Yellow Cab**.

---

## ğŸ“Œ **1. Modelos de RegresiÃ³n Lineal y LogÃ­stica**
### ğŸ” **RegresiÃ³n Lineal**
La **regresiÃ³n lineal** se utiliza para predecir una variable continua (como el precio de un viaje o el tiempo de duraciÃ³n).

âœ… **FÃ³rmula general:**  
\[
y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n
\]

---

### ğŸ“‹ **Ejemplo: PredicciÃ³n de tarifas usando RegresiÃ³n Lineal**
**Objetivo:** Predecir la tarifa (`fare_amount`) en funciÃ³n de la distancia del viaje (`trip_distance`).

```r
library(sparklyr)
library(dplyr)

# Conectar a Spark
sc <- spark_connect(master = "local[*]")

# Cargar el dataset
taxis_nyc <- spark_read_parquet(
  sc,
  name = "taxis_nyc",
  path = "yellow_tripdata_2023-01.parquet"
)

# Filtrar datos incorrectos
taxis_nyc <- taxis_nyc %>%
  filter(fare_amount > 0, trip_distance > 0)

# Entrenar el modelo de regresiÃ³n lineal
modelo_rl <- taxis_nyc %>%
  ml_linear_regression(fare_amount ~ trip_distance + passenger_count)

# Evaluar el modelo
predicciones <- ml_predict(modelo_rl, taxis_nyc)

# VisualizaciÃ³n
predicciones %>%
  select(fare_amount, prediction) %>%
  head(10) %>%
  collect()
```

âœ… **Ventaja:** FÃ¡cil de interpretar y eficiente en datasets grandes.  
â— **Desventaja:** Sensible a outliers y datos con relaciones no lineales.

---

### ğŸ” **RegresiÃ³n LogÃ­stica**
La **regresiÃ³n logÃ­stica** se utiliza para resolver problemas de **clasificaciÃ³n binaria** (por ejemplo, si una propina supera cierto umbral).

âœ… **FÃ³rmula general:**  
\[
p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_n X_n)}}
\]

---

### ğŸ“‹ **Ejemplo: ClasificaciÃ³n de viajes costosos (> $50)**
```r
# Variable objetivo: tarifa mayor o menor a $50
taxis_nyc <- taxis_nyc %>%
  mutate(fare_expensive = ifelse(fare_amount > 50, 1, 0))

# Entrenar el modelo de regresiÃ³n logÃ­stica
modelo_log <- taxis_nyc %>%
  ml_logistic_regression(fare_expensive ~ trip_distance + passenger_count)

# Evaluar el modelo
predicciones <- ml_predict(modelo_log, taxis_nyc)

# VisualizaciÃ³n de predicciones
predicciones %>%
  select(fare_expensive, prediction) %>%
  head(10) %>%
  collect()
```

âœ… **Ventaja:** Robusta para problemas de clasificaciÃ³n binaria.  
â— **Desventaja:** Menos eficiente en datasets con mÃºltiples clases.

---

## ğŸ“Œ **2. Modelos de ClasificaciÃ³n: Random Forest, Ãrboles de DecisiÃ³n y SVM**

### ğŸ” **a) Random Forest**
El algoritmo **Random Forest** es un modelo basado en mÃºltiples Ã¡rboles de decisiÃ³n que mejora la precisiÃ³n y reduce el riesgo de sobreajuste.

âœ… **Ventaja:** Robusto ante outliers y datos ruidosos.  
â— **Desventaja:** Puede ser costoso en tÃ©rminos computacionales.

---

### ğŸ“‹ **Ejemplo: ClasificaciÃ³n de tipo de pago usando Random Forest**
```r
# Entrenar un modelo Random Forest
modelo_rf <- taxis_nyc %>%
  ml_random_forest(payment_type ~ trip_distance + passenger_count)

# Evaluar el modelo
predicciones <- ml_predict(modelo_rf, taxis_nyc)

# VisualizaciÃ³n de predicciones
predicciones %>%
  select(payment_type, prediction) %>%
  head(10) %>%
  collect()
```

---

### ğŸ” **b) Ãrboles de DecisiÃ³n**
Los **Ãrboles de DecisiÃ³n** son modelos que dividen los datos en funciÃ³n de preguntas basadas en caracterÃ­sticas.

âœ… **Ventaja:** FÃ¡cil interpretaciÃ³n y visualizaciÃ³n.  
â— **Desventaja:** Pueden generar sobreajuste en datasets grandes.

---

### ğŸ“‹ **Ejemplo de Ãrbol de DecisiÃ³n**
```r
modelo_tree <- taxis_nyc %>%
  ml_decision_tree(payment_type ~ trip_distance + passenger_count)

ml_predict(modelo_tree, taxis_nyc) %>%
  select(payment_type, prediction) %>%
  head(10) %>%
  collect()
```

---

### ğŸ” **c) SVM (Support Vector Machines)**
Las **SVM** son eficaces para clasificar datos no lineales utilizando un **margen Ã³ptimo de separaciÃ³n**.

âœ… **Ventaja:** Eficiente en datasets grandes.  
â— **Desventaja:** Menos intuitivo que otros modelos.

---

### ğŸ“‹ **Ejemplo de SVM**
```r
modelo_svm <- taxis_nyc %>%
  ml_linear_svc(payment_type ~ trip_distance + passenger_count)

ml_predict(modelo_svm, taxis_nyc) %>%
  select(payment_type, prediction) %>%
  head(10) %>%
  collect()
```

---

## ğŸ“Œ **3. Modelos de Clustering: K-Means y GMM**

### ğŸ” **a) K-Means**
El algoritmo **K-Means** se utiliza para agrupar datos en clusters similares.

âœ… **Ventaja:** Muy eficiente en grandes volÃºmenes de datos.  
â— **Desventaja:** Sensible a la elecciÃ³n del nÃºmero de clusters (`k`).

---

### ğŸ“‹ **Ejemplo de Clustering con K-Means**
```r
modelo_kmeans <- taxis_nyc %>%
  ml_kmeans(features = c("trip_distance", "fare_amount"), k = 3)

# VisualizaciÃ³n de clusters
predicciones <- ml_predict(modelo_kmeans, taxis_nyc)

predicciones %>%
  select(trip_distance, fare_amount, prediction) %>%
  head(10) %>%
  collect()
```

---

### ğŸ” **b) Gaussian Mixture Model (GMM)**
El **GMM** es un modelo de clustering que asume que los datos provienen de mÃºltiples distribuciones normales.

âœ… **Ventaja:** Capta mejor la variabilidad de los datos.  
â— **Desventaja:** Menos eficiente que K-Means.

---

### ğŸ“‹ **Ejemplo con GMM**
```r
modelo_gmm <- taxis_nyc %>%
  ml_gaussian_mixture(features = c("trip_distance", "fare_amount"), k = 3)

ml_predict(modelo_gmm, taxis_nyc) %>%
  select(trip_distance, fare_amount, prediction) %>%
  head(10) %>%
  collect()
```

---

## ğŸ“š **6.7.4. EvaluaciÃ³n y ValidaciÃ³n de Modelos**
âœ… Uso de mÃ©tricas como **RMSE**, **MAE**, **AUC** y **F1 Score**.  
âœ… EvaluaciÃ³n de modelos usando `ml_evaluate()`.  
âœ… ImplementaciÃ³n de la validaciÃ³n cruzada en Spark con `ml_cross_validator()`.  
âœ… Ajuste de hiperparÃ¡metros mediante `ml_tuning()`.  

La **evaluaciÃ³n de modelos** es una fase crucial en el flujo de trabajo de Machine Learning, ya que permite medir el rendimiento del modelo y garantizar que sus predicciones son precisas y generalizables.

Spark, a travÃ©s de su paquete **`sparklyr`**, proporciona herramientas potentes para evaluar el rendimiento de modelos entrenados, validar su comportamiento en datos no vistos y optimizar sus hiperparÃ¡metros para mejorar la precisiÃ³n.

---

## ğŸ”¹ **Importancia de la EvaluaciÃ³n y ValidaciÃ³n**
âœ… Permite medir la precisiÃ³n y el error del modelo.  
âœ… Identifica si el modelo estÃ¡ sobreajustado (**overfitting**) o subajustado (**underfitting**).  
âœ… Facilita la elecciÃ³n del mejor modelo entre varias alternativas.  
âœ… Optimiza el rendimiento del modelo mediante la bÃºsqueda de los mejores hiperparÃ¡metros.  

---

## ğŸ“Œ **1. Uso de MÃ©tricas para Evaluar Modelos**
En Spark, se pueden aplicar diversas mÃ©tricas para evaluar modelos segÃºn el tipo de problema:

| **MÃ©trica** | **DescripciÃ³n** | **Uso recomendado** |
|:-------------|:-----------------|:---------------------|
| **RMSE** (Root Mean Squared Error) | Mide el error cuadrÃ¡tico medio en una escala real. | Modelos de regresiÃ³n. |
| **MAE** (Mean Absolute Error) | Mide el error medio absoluto, menos sensible a outliers que RMSE. | Modelos de regresiÃ³n. |
| **RÂ²** (R-Squared) | Mide quÃ© tan bien el modelo explica la variabilidad de los datos. | Modelos de regresiÃ³n. |
| **AUC** (Area Under Curve) | Mide la capacidad del modelo para distinguir entre clases. | Modelos de clasificaciÃ³n binaria. |
| **F1 Score** | Combina precisiÃ³n y exhaustividad en una Ãºnica mÃ©trica. | Modelos de clasificaciÃ³n multiclase. |

---

### ğŸ“‹ **Ejemplo: EvaluaciÃ³n con RMSE, MAE y RÂ² (RegresiÃ³n Lineal)**
**Objetivo:** Predecir la tarifa del viaje (`fare_amount`) en funciÃ³n de la distancia (`trip_distance`).

```r
library(sparklyr)
library(dplyr)

# ConexiÃ³n a Spark
sc <- spark_connect(master = "local[*]")

# Cargar el dataset NYC Yellow Cab
taxis_nyc <- spark_read_parquet(
  sc,
  name = "taxis_nyc",
  path = "yellow_tripdata_2023-01.parquet"
) %>%
  filter(fare_amount > 0, trip_distance > 0)

# Dividir los datos en entrenamiento y prueba
particiones <- sdf_partition(
  taxis_nyc,
  training = 0.8,
  testing = 0.2,
  seed = 1234
)

taxis_train <- particiones$training
taxis_test <- particiones$testing

# Entrenar el modelo de regresiÃ³n lineal
modelo_rl <- ml_linear_regression(
  taxis_train,
  fare_amount ~ trip_distance + passenger_count
)

# EvaluaciÃ³n del modelo
evaluacion <- ml_evaluate(modelo_rl, taxis_test)

# VisualizaciÃ³n de mÃ©tricas
evaluacion %>%
  select(rmse, mae, r2) %>%
  collect()
```

âœ… **RMSE bajo** indica que el modelo tiene menor error en las predicciones.  
âœ… **RÂ² cercano a 1** indica que el modelo explica bien la variabilidad de los datos.  

---

### ğŸ“‹ **Ejemplo: EvaluaciÃ³n con AUC y F1 Score (ClasificaciÃ³n)**
**Objetivo:** Clasificar si un viaje es costoso (`fare_amount > 50`) o no.

```r
# Crear una variable categÃ³rica binaria
taxis_nyc <- taxis_nyc %>%
  mutate(fare_expensive = ifelse(fare_amount > 50, 1, 0))

# Entrenar un modelo de regresiÃ³n logÃ­stica
modelo_log <- ml_logistic_regression(
  taxis_train,
  fare_expensive ~ trip_distance + passenger_count
)

# EvaluaciÃ³n del modelo
evaluacion_log <- ml_evaluate(modelo_log, taxis_test)

# VisualizaciÃ³n de mÃ©tricas
evaluacion_log %>%
  select(auc, f1) %>%
  collect()
```

âœ… **AUC alto** (> 0.8) indica que el modelo distingue bien entre clases.  
âœ… **F1 Score alto** indica que el modelo logra un buen equilibrio entre precisiÃ³n y exhaustividad.  

---

## ğŸ“Œ **2. EvaluaciÃ³n de Modelos usando `ml_evaluate()`**
La funciÃ³n **`ml_evaluate()`** permite evaluar automÃ¡ticamente modelos de regresiÃ³n y clasificaciÃ³n en Spark, proporcionando mÃ©tricas clave para tomar decisiones.

### ğŸ” **Principales mÃ©tricas que devuelve `ml_evaluate()`**
- Para **RegresiÃ³n**: RMSE, MAE, RÂ².  
- Para **ClasificaciÃ³n**: AUC, F1 Score, PrecisiÃ³n, Recall.  

---

### ğŸ“‹ **Ejemplo: EvaluaciÃ³n Completa de un Modelo de ClasificaciÃ³n**
```r
# EvaluaciÃ³n del modelo de regresiÃ³n logÃ­stica
evaluacion <- ml_evaluate(modelo_log, taxis_test)

# VisualizaciÃ³n completa
evaluacion %>%
  collect()
```

âœ… **Ventaja:** Facilita la obtenciÃ³n de mÃ©tricas sin necesidad de cÃ¡lculos manuales.  
âœ… **Flexible:** Se adapta automÃ¡ticamente al tipo de modelo (regresiÃ³n o clasificaciÃ³n).  

---

## ğŸ“Œ **3. ImplementaciÃ³n de ValidaciÃ³n Cruzada en Spark con `ml_cross_validator()`**
La **validaciÃ³n cruzada** es una tÃ©cnica avanzada que evalÃºa el rendimiento del modelo utilizando mÃºltiples particiones del dataset.

âœ… **Ventaja:** Proporciona una evaluaciÃ³n mÃ¡s robusta del modelo.  
â— **Desventaja:** Puede ser computacionalmente costoso en datasets muy grandes.

---

### ğŸ“‹ **Ejemplo: ValidaciÃ³n Cruzada en Spark**
```r
# Crear un grid de hiperparÃ¡metros para Random Forest
param_grid <- list(
  num_trees = c(10, 20, 50),
  max_depth = c(5, 10, 20)
)

# ValidaciÃ³n cruzada con bÃºsqueda de hiperparÃ¡metros
cv_model <- ml_cross_validator(
  taxis_train,
  estimator = ml_random_forest(payment_type ~ trip_distance + passenger_count),
  estimator_param_maps = param_grid,
  evaluator = ml_binary_classification_evaluator(),
  num_folds = 5
)

# Mejor modelo tras validaciÃ³n cruzada
best_model <- ml_fit(cv_model, taxis_train)

# EvaluaciÃ³n del modelo Ã³ptimo
ml_evaluate(best_model, taxis_test) %>%
  collect()
```

âœ… **Ventaja:** Garantiza una evaluaciÃ³n mÃ¡s precisa mediante mÃºltiples particiones del dataset.  
âœ… **Ideal para entornos Big Data**, donde una Ãºnica particiÃ³n de datos puede no representar bien el conjunto completo.  

---

## ğŸ“Œ **4. Ajuste de HiperparÃ¡metros mediante `ml_tuning()`**
El ajuste de hiperparÃ¡metros permite encontrar la combinaciÃ³n Ã³ptima de parÃ¡metros que maximiza el rendimiento del modelo.

âœ… **Ventaja:** Permite automatizar la bÃºsqueda de la mejor configuraciÃ³n del modelo.  
â— **Desventaja:** Puede requerir mayor tiempo de cÃ³mputo.

---

### ğŸ“‹ **Ejemplo: Ajuste de HiperparÃ¡metros en un Modelo de RegresiÃ³n Lineal**
```r
# Grid de hiperparÃ¡metros
param_grid <- list(
  reg_param = c(0.01, 0.1, 0.5),
  elastic_net_param = c(0.0, 0.5, 1.0)
)

# Modelo con ajuste de hiperparÃ¡metros
modelo_tuned <- ml_tuning(
  estimator = ml_linear_regression(fare_amount ~ trip_distance),
  estimator_param_maps = param_grid,
  evaluator = ml_regression_evaluator(),
  num_folds = 5
)

# Entrenamiento del modelo optimizado
best_model <- ml_fit(modelo_tuned, taxis_train)

# EvaluaciÃ³n del mejor modelo
ml_evaluate(best_model, taxis_test) %>%
  collect()
```

# ğŸ“š **Casos PrÃ¡cticos de Machine Learning con `sparklyr`**

En esta secciÃ³n desarrollaremos ejemplos prÃ¡cticos utilizando el dataset de **NYC Yellow Cab** en formato **Parquet**, junto con ejemplos adicionales para detecciÃ³n de fraude y sistemas de recomendaciÃ³n.

---

## ğŸ”¹ **1. DetecciÃ³n de Fraude usando Random Forest**
### ğŸš¨ **Objetivo:** Detectar posibles fraudes en los viajes de taxis en NYC utilizando un modelo de **Random Forest**.

### ğŸ“‹ **Dataset Recomendado:** [NYC Yellow Cab Data - Parquet](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet)

---

### ğŸ” **Paso 1: Cargar el Dataset y Filtrar Datos Irregulares**
```r
library(sparklyr)
library(dplyr)

# ConexiÃ³n a Spark
sc <- spark_connect(master = "local[*]")

# Cargar el dataset de NYC Yellow Cab
taxis_nyc <- spark_read_parquet(
  sc,
  name = "taxis_nyc",
  path = "yellow_tripdata_2023-01.parquet"
)

# Filtrar datos irregulares
taxis_nyc <- taxis_nyc %>%
  filter(fare_amount > 0, trip_distance > 0)

# Crear una variable binaria de fraude: 1 = Fraude, 0 = No fraude
taxis_nyc <- taxis_nyc %>%
  mutate(fraud = ifelse(fare_amount / trip_distance > 15, 1, 0))
```

---

### ğŸ” **Paso 2: Dividir los Datos en Entrenamiento y Prueba**
```r
particiones <- sdf_partition(
  taxis_nyc,
  training = 0.8,
  testing = 0.2,
  seed = 1234
)

taxis_train <- particiones$training
taxis_test <- particiones$testing
```

---

### ğŸ” **Paso 3: Entrenar el Modelo de Random Forest**
```r
modelo_rf <- ml_random_forest(
  taxis_train,
  fraud ~ trip_distance + fare_amount + passenger_count
)
```

---

### ğŸ” **Paso 4: Evaluar el Modelo**
```r
predicciones <- ml_predict(modelo_rf, taxis_test)

# MÃ©tricas de evaluaciÃ³n
evaluacion <- ml_evaluate(modelo_rf, taxis_test)

# Mostrar el AUC y el F1 Score
evaluacion %>%
  select(auc, f1) %>%
  collect()
```

âœ… **ConclusiÃ³n:** El modelo identifica viajes sospechosos basados en caracterÃ­sticas anÃ³malas, como tarifas elevadas en distancias cortas.

---

## ğŸ”¹ **2. PredicciÃ³n de Tarifas de Taxis usando RegresiÃ³n Lineal**
### ğŸš¨ **Objetivo:** Predecir la tarifa (`fare_amount`) en funciÃ³n de la distancia y el nÃºmero de pasajeros.

---

### ğŸ” **Paso 1: Cargar el Dataset**
```r
# Cargar el dataset
taxis_nyc <- spark_read_parquet(
  sc,
  name = "taxis_nyc",
  path = "yellow_tripdata_2023-01.parquet"
) %>%
  filter(fare_amount > 0, trip_distance > 0)
```

---

### ğŸ” **Paso 2: Entrenar el Modelo de RegresiÃ³n Lineal**
```r
modelo_rl <- ml_linear_regression(
  taxis_nyc,
  fare_amount ~ trip_distance + passenger_count
)
```

---

### ğŸ” **Paso 3: Evaluar el Modelo**
```r
evaluacion <- ml_evaluate(modelo_rl, taxis_nyc)

# VisualizaciÃ³n de mÃ©tricas
evaluacion %>%
  select(rmse, r2) %>%
  collect()
```

---

### ğŸ” **Paso 3: Generar Recomendaciones**
```r
# Generar recomendaciones para todos los usuarios
recomendaciones <- ml_recommend(modelo_als, type = "items", num = 5)

# Visualizar recomendaciones
recomendaciones %>%
  head(10) %>%
  collect()
```

âœ… **ConclusiÃ³n:** Este modelo recomienda productos especÃ­ficos a cada cliente basÃ¡ndose en su historial de compras.

---

## ğŸ”¹ **4. SegmentaciÃ³n de Clientes usando K-Means**
### ğŸš¨ **Objetivo:** Identificar grupos de pasajeros con comportamientos similares en funciÃ³n de la distancia y la tarifa del viaje.

---

### ğŸ” **Paso 1: Cargar el Dataset**
```r
# Cargar el dataset
taxis_nyc <- spark_read_parquet(
  sc,
  name = "taxis_nyc",
  path = "yellow_tripdata_2023-01.parquet"
)
```

---

### ğŸ” **Paso 2: Entrenar el Modelo K-Means**
```r
modelo_kmeans <- ml_kmeans(
  taxis_nyc,
  features = c("trip_distance", "fare_amount"),
  k = 3
)
```
---

### ğŸ” **Paso 3: VisualizaciÃ³n de los Clusters**
```r
# Predecir los clusters
clusters <- ml_predict(modelo_kmeans, taxis_nyc)

# Visualizar algunos registros con sus clusters asignados
clusters %>%
  select(trip_distance, fare_amount, prediction) %>%
  head(10) %>%
  collect()
```



