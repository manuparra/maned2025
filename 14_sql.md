# ğŸ”µ **Tema 6.5.4 â€“ ManipulaciÃ³n y Consulta de Datos con Spark SQL**

La manipulaciÃ³n y consulta de datos es una parte esencial en el manejo de **Big Data**. Para trabajar con grandes volÃºmenes de datos de forma eficiente, se requiere el uso de herramientas optimizadas que permitan realizar consultas avanzadas en entornos distribuidos.

En esta secciÃ³n aprenderÃ¡s a:

âœ… Entender quÃ© es Spark SQL y cÃ³mo se integra con `sparklyr` en R  
âœ… Realizar consultas bÃ¡sicas y avanzadas en Spark SQL  
âœ… Implementar operaciones comunes como `GROUP BY`, `JOIN`, `ORDER BY`, etc.  
âœ… Ejecutar consultas sobre el dataset real de **NYC Taxi Trips**  
âœ… Aplicar tÃ©cnicas avanzadas como subconsultas, ventanas y funciones analÃ­ticas  
âœ… Resolver problemas reales usando Spark SQL en un entorno empresarial  

---

# ğŸ”¹ **1. Â¿QuÃ© es Spark SQL?**

**Spark SQL** es un mÃ³dulo de Apache Spark que proporciona una interfaz basada en **SQL** para consultar datos distribuidos de forma eficiente. Permite ejecutar sentencias SQL estÃ¡ndar sobre datos almacenados en Spark, ya sea en formato **Parquet**, **CSV**, **JSON** o en sistemas distribuidos como **HDFS**, **S3**, etc.

### ğŸ” **Ventajas de Spark SQL**
âœ… Permite realizar consultas avanzadas utilizando sintaxis SQL estÃ¡ndar  
âœ… Soporta una amplia variedad de fuentes de datos (Parquet, JSON, Avro, etc.)  
âœ… OptimizaciÃ³n automÃ¡tica del plan de ejecuciÃ³n mediante el **Catalyst Optimizer**  
âœ… IntegraciÃ³n perfecta con `dplyr`, permitiendo manipulaciÃ³n hÃ­brida en SQL y R  
âœ… Escalable para manejar volÃºmenes masivos de datos  

---

# ğŸ”¹ **2. ConfiguraciÃ³n inicial de Spark SQL en R con `sparklyr`**

Para ejecutar consultas SQL desde R utilizando Spark SQL, se requiere configurar la conexiÃ³n usando `sparklyr`.

### ğŸ” **ConfiguraciÃ³n de Spark con R**
```r
# Instalar librerÃ­as necesarias
library(sparklyr)
library(dplyr)

# ConexiÃ³n a Spark en modo local
sc <- spark_connect(master = "local")

# Cargar el dataset de NYC Taxi Trips (Parquet)
taxi_nyc <- spark_read_parquet(sc,
                               name = "taxi_nyc",
                               path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet")

# Verificar las primeras filas
taxi_nyc %>% head() %>% collect()
```

---

# ğŸ”¹ **3. Consultas BÃ¡sicas en Spark SQL**

## ğŸ“Œ **3.1. Sintaxis bÃ¡sica en Spark SQL**
La sintaxis de Spark SQL es muy similar a SQL estÃ¡ndar. Ejemplos de consultas bÃ¡sicas incluyen:

âœ… `SELECT` â€“ Para seleccionar columnas especÃ­ficas  
âœ… `WHERE` â€“ Para filtrar registros segÃºn condiciones  
âœ… `ORDER BY` â€“ Para ordenar los datos segÃºn criterios  
âœ… `LIMIT` â€“ Para limitar el nÃºmero de resultados  

---

## ğŸ” **3.2. Ejemplos bÃ¡sicos con el dataset NYC Taxi Trips**

### â¤ **Seleccionar columnas especÃ­ficas**
```r
# Mostrar las columnas 'pickup_datetime', 'fare_amount' y 'tip_amount'
dbGetQuery(sc, "
  SELECT 
    pickup_datetime,
    fare_amount,
    tip_amount
  FROM taxi_nyc
  LIMIT 10
")
```

---

### â¤ **Filtrar registros segÃºn una condiciÃ³n**
Por ejemplo, filtrar viajes con tarifas superiores a 50 USD:

```r
dbGetQuery(sc, "
  SELECT 
    pickup_datetime,
    fare_amount
  FROM taxi_nyc
  WHERE fare_amount > 50
  LIMIT 10
")
```

---

### â¤ **Ordenar los datos por un criterio especÃ­fico**
Ordenar por tarifas mÃ¡s elevadas:

```r
dbGetQuery(sc, "
  SELECT 
    pickup_datetime,
    fare_amount
  FROM taxi_nyc
  ORDER BY fare_amount DESC
  LIMIT 10
")
```

---

### â¤ **Calcular estadÃ­sticas bÃ¡sicas**
Por ejemplo, calcular la tarifa promedio de todos los viajes.

```r
dbGetQuery(sc, "
  SELECT 
    AVG(fare_amount) AS tarifa_promedio
  FROM taxi_nyc
")
```

---

# ğŸ”¹ **4. Consultas Avanzadas en Spark SQL**

Las consultas avanzadas en Spark SQL permiten realizar anÃ¡lisis mÃ¡s complejos mediante el uso de:

âœ… **`GROUP BY`** para agrupaciones  
âœ… **`JOIN`** para combinar tablas  
âœ… **`WINDOW FUNCTIONS`** para cÃ¡lculos avanzados sobre particiones de datos  
âœ… **`SUBQUERIES`** para cÃ¡lculos condicionales o anidados  

---

## ğŸ“Œ **4.1. Agrupaciones con `GROUP BY`**

### â¤ **Calcular el total de ingresos por dÃ­a**
```r
dbGetQuery(sc, "
  SELECT 
    DATE(pickup_datetime) AS fecha,
    SUM(fare_amount) AS total_ingresos
  FROM taxi_nyc
  GROUP BY fecha
  ORDER BY fecha ASC
")
```

---

### â¤ **Calcular la tarifa promedio por mÃ©todo de pago**
```r
dbGetQuery(sc, "
  SELECT 
    payment_type,
    AVG(fare_amount) AS tarifa_media
  FROM taxi_nyc
  GROUP BY payment_type
")
```

---

## ğŸ“Œ **4.2. Uniones con `JOIN`**

Spark SQL permite combinar tablas de forma eficiente utilizando `JOIN`.

### â¤ **Unir la tabla de taxis con una tabla de zonas**
```r
zonas <- data.frame(
  zone_id = c(1, 2, 3),
  zone_name = c("Manhattan", "Brooklyn", "Queens")
)

# Cargar la tabla de zonas en Spark
sdf_copy_to(sc, zonas, name = "zonas", overwrite = TRUE)

# JOIN para combinar ambas tablas
dbGetQuery(sc, "
  SELECT 
    taxi_nyc.PULocationID,
    zonas.zone_name,
    AVG(taxi_nyc.fare_amount) AS tarifa_promedio
  FROM taxi_nyc
  JOIN zonas
    ON taxi_nyc.PULocationID = zonas.zone_id
  GROUP BY zonas.zone_name
")
```

---

## ğŸ“Œ **4.3. Funciones de Ventana (`WINDOW FUNCTIONS`)**

Las funciones de ventana permiten calcular mÃ©tricas avanzadas sobre subconjuntos de datos.

### â¤ **Calcular la tarifa acumulada por dÃ­a**
```r
dbGetQuery(sc, "
  SELECT 
    pickup_datetime,
    fare_amount,
    SUM(fare_amount) OVER (PARTITION BY DATE(pickup_datetime)) AS tarifa_acumulada
  FROM taxi_nyc
  ORDER BY pickup_datetime ASC
  LIMIT 10
")
```

---

## ğŸ“Œ **4.4. Subconsultas (`SUBQUERIES`)**

Las subconsultas permiten anidar consultas para resolver problemas complejos.

### â¤ **Obtener las 5 tarifas mÃ¡s altas de cada dÃ­a**
```r
dbGetQuery(sc, "
  SELECT *
  FROM (
    SELECT 
      pickup_datetime,
      fare_amount,
      RANK() OVER (PARTITION BY DATE(pickup_datetime) ORDER BY fare_amount DESC) AS rank
    FROM taxi_nyc
  ) AS ranked
  WHERE rank <= 5
")
```

---

# ğŸ”¹ **5. Ejercicios PrÃ¡cticos**

### ğŸŸ© **Ejercicio 1 â€“ AnÃ¡lisis de Propinas por Zonas**
**Objetivo:**  
- Cargar el dataset de NYC Taxi Trips.  
- Calcular la propina media (`tip_amount`) por cada zona.  
- Identificar la zona que registra la mayor propina promedio.  

**CÃ³digo:**

```r

```

---

### ğŸŸ© **Ejercicio 2 â€“ AnÃ¡lisis de Viajes Largos**
**Objetivo:**  
- Identificar los viajes cuya distancia (`trip_distance`) sea superior a **30 millas**.  
- Determinar la tarifa promedio para estos viajes.  

**CÃ³digo:**

```r

```
