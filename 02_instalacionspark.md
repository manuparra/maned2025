#  Instalaci贸n y Configuraci贸n de Apache Spark para R

---

##  **1. Instalaci贸n de Spark en R**
Para instalar Spark en R, abre **RStudio o Jupyter Lab** y ejecuta:

```r
# Instalar la librer铆a sparklyr
install.packages("sparklyr")

# Instalar Apache Spark en local (descarga autom谩tica)
sparklyr::spark_install()
```

### ** Notas sobre la instalaci贸n**
- `sparklyr` permite la conexi贸n entre **R y Apache Spark**.
- `spark_install()` descarga e instala Spark autom谩ticamente en el sistema.

Si prefieres instalar **una versi贸n espec铆fica** de Spark, usa:

```r
sparklyr::spark_install(version = "3.2.0")
```

---

##  **2. Verificaci贸n de la instalaci贸n**
Para asegurarte de que Spark est谩 correctamente instalado, ejecuta:

```r
library(sparklyr)
spark_available_versions()
```

Para comprobar que Spark se inicia correctamente, conecta R con Spark:

```r
sc <- spark_connect(master = "local")
sc
```

Si la conexi贸n es exitosa, ver谩s informaci贸n sobre el servidor Spark.

---

##  **3. Configuraci贸n avanzada de Spark**
Si necesitas asignar m谩s recursos a Spark, puedes personalizar la inicializaci贸n:

```r
sc <- spark_connect(master = "local", 
                    config = list(spark.memory.fraction = 0.8, 
                                  spark.executor.memory = "4G"))
```

O si est谩s trabajando con **Spark en un cl煤ster**, cambia el `master`:

```r
sc <- spark_connect(master = "yarn")  # Para entornos Hadoop/YARN
sc <- spark_connect(master = "spark://ip-servidor:7077")  # Para Spark independiente
```

---

##  **4. Uso b谩sico de Spark con R**
Una vez conectado, puedes cargar y manipular datos directamente en Spark:

```r
# Cargar datos en Spark desde un CSV
datos <- spark_read_csv(sc, name = "datos_spark", path = "ruta/dataset.csv", infer_schema = TRUE)

# Ver las primeras filas
head(datos)

# Transformaciones en Spark usando dplyr
library(dplyr)

datos %>%
  group_by(categoria) %>%
  summarise(promedio = mean(valor)) %>%
  collect()  # 'collect()' trae los resultados a R
```

---

##  **5. Apagar la sesi贸n de Spark y desinsta**
Cuando termines de usar Spark, cierra la conexi贸n con:

```r
spark_disconnect(sc)
```
Con esto se liberar谩n todos los recursos de memoria, disco, etc. asociados a la session de spark.

Para desinstalar spark completamente usa lo siguiente:

```r
spark_uninstall(version = "3.2.0")
```
