# ğŸ”µ **Tema 6.5.2 â€“ Carga e Ingesta de Datos en Big Data**

La **ingesta de datos** es el primer paso clave en el flujo de trabajo de Big Data. Este proceso consiste en capturar, transformar y almacenar los datos provenientes de diversas fuentes para que puedan ser utilizados posteriormente en anÃ¡lisis o modelos predictivos.

En esta secciÃ³n aprenderemos a:

âœ… Comprender los conceptos de ETL y ELT  
âœ… Realizar la carga de datos desde distintos formatos como **CSV** y **Parquet**  
âœ… Desarrollar un flujo de trabajo **ETL** (Extract, Transform, Load)  
âœ… Desarrollar un flujo de trabajo **ELT** (Extract, Load, Transform)  
âœ… Aplicar ejemplos prÃ¡cticos con el dataset de **NYC Taxi Trips**  

---

## ğŸ”¹ **1. Conceptos clave: ETL vs ELT**

### ğŸ“‚ **Â¿QuÃ© es ETL (Extract, Transform, Load)?**

El flujo **ETL** consiste en:

1. **Extract (ExtracciÃ³n):** Se obtienen datos desde mÃºltiples fuentes (bases de datos, APIs, logs, etc.).
2. **Transform (TransformaciÃ³n):** Se limpian, filtran o enriquecen los datos antes de almacenarlos.
3. **Load (Carga):** Los datos transformados se almacenan en un almacÃ©n de datos (Data Lake o Data Warehouse).

âœ… **Ventaja:** Los datos llegan limpios y organizados.  
âŒ **Inconveniente:** Si los datos crecen en volumen, la transformaciÃ³n previa puede volverse lenta.

---

### ğŸ“‚ **Â¿QuÃ© es ELT (Extract, Load, Transform)?**

El flujo **ELT** es una variante moderna en la que:

1. **Extract (ExtracciÃ³n):** Los datos se extraen desde diversas fuentes.  
2. **Load (Carga):** Los datos se cargan directamente en el sistema de almacenamiento distribuido (HDFS, S3, GCS, etc.).  
3. **Transform (TransformaciÃ³n):** La limpieza, normalizaciÃ³n y enriquecimiento se realiza dentro del sistema de almacenamiento.

âœ… **Ventaja:** Escalable y eficiente para grandes volÃºmenes.  
âŒ **Inconveniente:** Los datos llegan "en bruto" y requieren mÃ¡s pasos posteriores.

---

### ğŸ” **Â¿CuÃ¡ndo usar cada modelo?**

- **ETL**: Cuando se trabaja con datos mÃ¡s pequeÃ±os o se requiere que los datos estÃ©n limpios antes de su almacenamiento.  
- **ELT**: MÃ¡s eficiente para grandes volÃºmenes de datos ya que se aprovecha la escalabilidad del entorno distribuido.

---

## ğŸ”¹ **2. Ejemplo de carga de datos en Spark con `sparklyr`**

El dataset de taxis de Nueva York estÃ¡ disponible tanto en formato **Parquet** (eficiente para Big Data) como en **CSV** (mÃ¡s tradicional).

---

### ğŸŸ  **Carga de datos en formato Parquet** (Recomendado para Big Data)
El formato **Parquet** es altamente eficiente porque almacena los datos de forma columnar y comprimida, lo que mejora el rendimiento en consultas analÃ­ticas.

```r
# LibrerÃ­as necesarias
library(sparklyr)
library(dplyr)

# ConexiÃ³n a Spark
sc <- spark_connect(master = "local")

# Carga del dataset en formato Parquet
taxi_nyc <- spark_read_parquet(
  sc, 
  name = "taxi_nyc",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
)

# Ver primeras filas del dataset
taxi_nyc %>% head(10) %>% collect()
```

---

### ğŸŸ  **Carga de datos en formato CSV** (ComÃºn en entornos empresariales)
El formato **CSV** es ampliamente utilizado, pero menos eficiente que Parquet para Big Data debido a la falta de compresiÃ³n e indexaciÃ³n.

```r
# Carga del dataset en formato CSV
taxi_nyc_csv <- spark_read_csv(
  sc,
  name = "taxi_nyc_csv",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.csv",
  infer_schema = TRUE,
  header = TRUE
)

# Ver primeras filas del dataset CSV
taxi_nyc_csv %>% head(10) %>% collect()
```

---

## ğŸ”¹ **3. Flujo ETL con el dataset NYC Taxi**

En este flujo, se extraerÃ¡n datos en formato CSV, se limpiarÃ¡n y transformarÃ¡n dentro de Spark, y finalmente se almacenarÃ¡n en formato Parquet para su posterior anÃ¡lisis.

### ğŸ”¹ **Paso 1: ExtracciÃ³n**
Se extraen los datos desde un archivo CSV.

```r
# ExtracciÃ³n de datos (EXTRACT)
taxi_nyc_csv <- spark_read_csv(
  sc,
  name = "taxi_nyc_csv",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.csv",
  infer_schema = TRUE,
  header = TRUE
)
```

---

### ğŸ”¹ **Paso 2: TransformaciÃ³n**
- EliminaciÃ³n de registros con tarifas negativas
- CreaciÃ³n de una nueva columna "tarifa_total" que sume tarifa base + propina

```r
# TransformaciÃ³n de datos (TRANSFORM)
taxi_nyc_clean <- taxi_nyc_csv %>%
  filter(fare_amount > 0) %>%
  mutate(total_fare = fare_amount + tip_amount)
```

---

### ğŸ”¹ **Paso 3: Carga**
Se almacenan los datos limpios en formato Parquet para mejorar el rendimiento en consultas futuras.

```r
# Carga de datos (LOAD)
spark_write_parquet(taxi_nyc_clean, path = "hdfs://ruta/nyc_taxi_clean.parquet")
```

---

## ğŸ”¹ **4. Flujo ELT con el dataset NYC Taxi**

En este flujo, se cargan primero los datos en bruto dentro de Spark (en formato Parquet) y posteriormente se realiza la transformaciÃ³n directamente en Spark.

### ğŸ”¹ **Paso 1: ExtracciÃ³n y Carga**
Carga directa de datos sin limpieza previa.

```r
# ExtracciÃ³n y Carga simultÃ¡nea (EXTRACT & LOAD)
taxi_nyc_raw <- spark_read_parquet(
  sc, 
  name = "taxi_nyc_raw",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
)
```

---

### ğŸ”¹ **Paso 2: TransformaciÃ³n**
TransformaciÃ³n dentro del entorno distribuido Spark.

```r
# TransformaciÃ³n interna en Spark
taxi_nyc_clean_elt <- taxi_nyc_raw %>%
  filter(fare_amount > 0) %>%
  mutate(total_fare = fare_amount + tip_amount)

# Carga final del resultado en Parquet optimizado
spark_write_parquet(taxi_nyc_clean_elt, path = "hdfs://ruta/nyc_taxi_clean_elt.parquet")
```

---

## ğŸ”¹ **5. Ejercicios prÃ¡cticos**

### ğŸŸ© **Ejercicio 1 â€“ AnÃ¡lisis de tarifas limpias usando ETL**
- Carga el dataset de taxis en formato **CSV**.
- Filtra los registros con tarifas negativas o nulas.
- Calcula la tarifa media y la tarifa mÃ¡xima.
- Almacena el resultado en formato **Parquet**.

```r

```

---

### ğŸŸ© **Ejercicio 2 â€“ AnÃ¡lisis de viajes por hora usando ELT**
- Carga el dataset de taxis en formato **Parquet**.
- Crea una columna que extraiga la **hora** de recogida (`pickup_datetime`).
- Calcula el total de viajes por hora.
- Almacena el resultado en formato **Parquet**.


```r

```
