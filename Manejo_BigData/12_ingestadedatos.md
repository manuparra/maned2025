# ğŸ”µ **Tema 6.5.2 â€“ Carga e Ingesta de Datos en Big Data**

La **ingesta de datos** es el primer paso clave en el flujo de trabajo de Big Data. Este proceso consiste en capturar, transformar y almacenar los datos provenientes de diversas fuentes para que puedan ser utilizados posteriormente en anÃ¡lisis o modelos predictivos.

En esta secciÃ³n aprenderemos a:

âœ… Comprender los conceptos de ETL y ELT  
âœ… Realizar la carga de datos desde distintos formatos como **CSV** y **Parquet**  
âœ… Desarrollar un flujo de trabajo **ETL** (Extract, Transform, Load)  
âœ… Desarrollar un flujo de trabajo **ELT** (Extract, Load, Transform)  
âœ… Aplicar ejemplos prÃ¡cticos con el dataset de **NYC Taxi Trips**  

---

## ğŸ”¹ **1. Conceptos clave: ETL vs ELT**

### ğŸ“‚ **Â¿QuÃ© es ETL (Extract, Transform, Load)?**

El flujo **ETL** consiste en:

1. **Extract (ExtracciÃ³n):** Se obtienen datos desde mÃºltiples fuentes (bases de datos, APIs, logs, etc.).
2. **Transform (TransformaciÃ³n):** Se limpian, filtran o enriquecen los datos antes de almacenarlos.
3. **Load (Carga):** Los datos transformados se almacenan en un almacÃ©n de datos (Data Lake o Data Warehouse).

âœ… **Ventaja:** Los datos llegan limpios y organizados.  
âŒ **Inconveniente:** Si los datos crecen en volumen, la transformaciÃ³n previa puede volverse lenta.

---

### ğŸ“‚ **Â¿QuÃ© es ELT (Extract, Load, Transform)?**

El flujo **ELT** es una variante moderna en la que:

1. **Extract (ExtracciÃ³n):** Los datos se extraen desde diversas fuentes.  
2. **Load (Carga):** Los datos se cargan directamente en el sistema de almacenamiento distribuido (HDFS, S3, GCS, etc.).  
3. **Transform (TransformaciÃ³n):** La limpieza, normalizaciÃ³n y enriquecimiento se realiza dentro del sistema de almacenamiento.

âœ… **Ventaja:** Escalable y eficiente para grandes volÃºmenes.  
âŒ **Inconveniente:** Los datos llegan "en bruto" y requieren mÃ¡s pasos posteriores.

---

### ğŸ” **Â¿CuÃ¡ndo usar cada modelo?**

- **ETL**: Cuando se trabaja con datos mÃ¡s pequeÃ±os o se requiere que los datos estÃ©n limpios antes de su almacenamiento.  
- **ELT**: MÃ¡s eficiente para grandes volÃºmenes de datos ya que se aprovecha la escalabilidad del entorno distribuido.

---

## ğŸ”¹ **2. Ejemplo de carga de datos en Spark con `sparklyr`**

El dataset de taxis de Nueva York estÃ¡ disponible tanto en formato **Parquet** (eficiente para Big Data) como en **CSV** (mÃ¡s tradicional).

---

### ğŸŸ  **Carga de datos en formato Parquet** (Recomendado para Big Data)
El formato **Parquet** es altamente eficiente porque almacena los datos de forma columnar y comprimida, lo que mejora el rendimiento en consultas analÃ­ticas.

```r
# LibrerÃ­as necesarias
library(sparklyr)
library(dplyr)

# ConexiÃ³n a Spark
sc <- spark_connect(master = "local")

# Carga del dataset en formato Parquet
taxi_nyc <- spark_read_parquet(
  sc, 
  name = "taxi_nyc",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
)

# Ver primeras filas del dataset
taxi_nyc %>% head(10) %>% collect()
```

---

### ğŸŸ  **Carga de datos en formato CSV** (ComÃºn en entornos empresariales)
El formato **CSV** es ampliamente utilizado, pero menos eficiente que Parquet para Big Data debido a la falta de compresiÃ³n e indexaciÃ³n.

```r
# Carga del dataset en formato CSV
taxi_nyc_csv <- spark_read_csv(
  sc,
  name = "taxi_nyc_csv",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.csv",
  infer_schema = TRUE,
  header = TRUE
)

# Ver primeras filas del dataset CSV
taxi_nyc_csv %>% head(10) %>% collect()
```

---

## ğŸ”¹ **3. Flujo ETL con el dataset NYC Taxi**

En este flujo, se extraerÃ¡n datos en formato CSV, se limpiarÃ¡n y transformarÃ¡n dentro de Spark, y finalmente se almacenarÃ¡n en formato Parquet para su posterior anÃ¡lisis.

### ğŸ”¹ **Paso 1: ExtracciÃ³n**
Se extraen los datos desde un archivo CSV.

```r
# ExtracciÃ³n de datos (EXTRACT)
taxi_nyc_csv <- spark_read_csv(
  sc,
  name = "taxi_nyc_csv",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.csv",
  infer_schema = TRUE,
  header = TRUE
)
```

---

### ğŸ”¹ **Paso 2: TransformaciÃ³n**
- EliminaciÃ³n de registros con tarifas negativas
- CreaciÃ³n de una nueva columna "tarifa_total" que sume tarifa base + propina

```r
# TransformaciÃ³n de datos (TRANSFORM)
taxi_nyc_clean <- taxi_nyc_csv %>%
  filter(fare_amount > 0) %>%
  mutate(total_fare = fare_amount + tip_amount)
```

---

### ğŸ”¹ **Paso 3: Carga**
Se almacenan los datos limpios en formato Parquet para mejorar el rendimiento en consultas futuras.

```r
# Carga de datos (LOAD)
spark_write_parquet(taxi_nyc_clean, path = "hdfs://ruta/nyc_taxi_clean.parquet")
```

---

## ğŸ”¹ **4. Flujo ELT con el dataset NYC Taxi**

En este flujo, se cargan primero los datos en bruto dentro de Spark (en formato Parquet) y posteriormente se realiza la transformaciÃ³n directamente en Spark.

### ğŸ”¹ **Paso 1: ExtracciÃ³n y Carga**
Carga directa de datos sin limpieza previa.

```r
# ExtracciÃ³n y Carga simultÃ¡nea (EXTRACT & LOAD)
taxi_nyc_raw <- spark_read_parquet(
  sc, 
  name = "taxi_nyc_raw",
  path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet"
)
```

---

### ğŸ”¹ **Paso 2: TransformaciÃ³n**
TransformaciÃ³n dentro del entorno distribuido Spark.

```r
# TransformaciÃ³n interna en Spark
taxi_nyc_clean_elt <- taxi_nyc_raw %>%
  filter(fare_amount > 0) %>%
  mutate(total_fare = fare_amount + tip_amount)

# Carga final del resultado en Parquet optimizado
spark_write_parquet(taxi_nyc_clean_elt, path = "hdfs://ruta/nyc_taxi_clean_elt.parquet")
```

---
# ğŸ” **Modos de Inferir el Esquema (Schema) de un CSV en Apache Spark y su Eficiencia**

Cuando se trabaja con datos en formato **CSV** en Apache Spark, uno de los pasos mÃ¡s importantes es la **definiciÃ³n del esquema** (schema). Un esquema describe la estructura de los datos, es decir, el nombre de las columnas, sus tipos de datos (entero, cadena, fecha, etc.) y las posibles restricciones.  

En Apache Spark existen diversas formas de inferir o definir el esquema de un CSV, cada una con sus ventajas y desventajas en tÃ©rminos de **eficiencia**, **precisiÃ³n** y **rendimiento**.

---

## ğŸ”¹ **Â¿Por quÃ© es importante el esquema en Spark?**
Dado que Spark es una plataforma **distribuida**, los datos se procesan en paralelo en mÃºltiples nodos. Inferir el esquema correctamente es fundamental para:

âœ… Evitar errores en el tratamiento de datos.  
âœ… Mejorar el rendimiento, ya que Spark puede optimizar las consultas cuando conoce los tipos de datos.  
âœ… Reducir el tiempo de carga al evitar que Spark tenga que inspeccionar cada fila del CSV.  

---

## ğŸ”¹ **Modos de Inferir el Esquema en Apache Spark**

Existen principalmente tres mÃ©todos para inferir o definir el esquema de un CSV en Spark:

1. **Inferencia AutomÃ¡tica del Esquema**  
2. **EspecificaciÃ³n Manual del Esquema**  
3. **Lectura Parcial del Dataset (Sampling)**  

---

## ğŸ“Œ **1. Inferencia AutomÃ¡tica del Esquema**
La inferencia automÃ¡tica es el mÃ©todo mÃ¡s sencillo, pero puede ser menos eficiente en grandes volÃºmenes de datos.

### ğŸ” **Â¿CÃ³mo funciona?**
Cuando se utiliza la opciÃ³n **`infer_schema = TRUE`** (en `spark_read_csv()` de `sparklyr`), Spark inspecciona una parte del archivo para determinar el tipo de datos de cada columna.

```r
library(sparklyr)
library(dplyr)

# Conectar a Spark
sc <- spark_connect(master = "local")

# Inferencia automÃ¡tica del esquema
taxi_nyc <- spark_read_csv(
  sc,
  name = "taxi_nyc",
  path = "hdfs://ruta/dataset.csv",
  infer_schema = TRUE,
  header = TRUE
)

# VisualizaciÃ³n del esquema inferido
sdf_schema(taxi_nyc)
```

### âœ… **Ventajas**  
âœ”ï¸ RÃ¡pido y sencillo para conjuntos de datos pequeÃ±os o medianos.  
âœ”ï¸ No requiere que el usuario defina manualmente el esquema.  

### â— **Desventajas**  
âŒ Puede ser **lento** en conjuntos de datos grandes porque Spark analiza mÃºltiples filas para determinar los tipos de datos.  
âŒ El resultado puede ser **impreciso** si los datos contienen valores atÃ­picos o errores.  

> âš ï¸ **RecomendaciÃ³n:** En archivos muy grandes, este mÃ©todo no es Ã³ptimo, ya que Spark debe escanear el dataset para inferir correctamente los tipos de datos.

---

## ğŸ“Œ **2. EspecificaciÃ³n Manual del Esquema**
La definiciÃ³n manual del esquema es la opciÃ³n **mÃ¡s eficiente** y precisa, especialmente cuando se conoce de antemano la estructura del CSV.

### ğŸ” **Â¿CÃ³mo funciona?**
Se utiliza el parÃ¡metro **`columns`** en `spark_read_csv()` para definir el esquema.

### ğŸ” **Ejemplo de definiciÃ³n manual del esquema**
```r
# DefiniciÃ³n manual del esquema
schema_manual <- list(
  VendorID = "integer",
  pickup_datetime = "timestamp",
  dropoff_datetime = "timestamp",
  passenger_count = "integer",
  trip_distance = "double",
  fare_amount = "double",
  tip_amount = "double",
  total_amount = "double"
)

# Carga del CSV con el esquema manual
taxi_nyc <- spark_read_csv(
  sc,
  name = "taxi_nyc",
  path = "hdfs://ruta/dataset.csv",
  columns = schema_manual,
  header = TRUE
)

# VisualizaciÃ³n del esquema definido
sdf_schema(taxi_nyc)
```

### âœ… **Ventajas**  
âœ”ï¸ **Mayor eficiencia:** Spark no necesita escanear el dataset, ya que el esquema estÃ¡ definido.  
âœ”ï¸ Garantiza que los tipos de datos sean correctos.  
âœ”ï¸ Reduce el riesgo de errores de inferencia.  

### â— **Desventajas**  
âŒ Requiere que el usuario conozca previamente la estructura del dataset.  
âŒ Si el CSV tiene columnas adicionales o nombres inconsistentes, pueden generarse errores.

> ğŸ” **Consejo:** Este mÃ©todo es ideal para datasets muy grandes o cuando el esquema es conocido de antemano.

---

## ğŸ“Œ **3. Inferencia del Esquema mediante Muestreo (Sampling)**
La tÃ©cnica de **muestreo** permite que Spark examine solo una parte del CSV para inferir el esquema, en lugar de inspeccionar todas las filas.

### ğŸ” **Â¿CÃ³mo funciona?**
Con el parÃ¡metro **`samplingRatio`**, puedes especificar el porcentaje de datos que Spark inspeccionarÃ¡ para inferir el esquema.

### ğŸ” **Ejemplo de inferencia mediante muestreo**
```r
# Inferencia del esquema con muestreo del 10%
taxi_nyc <- spark_read_csv(
  sc,
  name = "taxi_nyc",
  path = "hdfs://ruta/dataset.csv",
  infer_schema = TRUE,
  header = TRUE,
  options = list(samplingRatio = 0.1)  # Muestreo del 10%
)
```

### âœ… **Ventajas**  
âœ”ï¸ MÃ¡s rÃ¡pido que la inferencia automÃ¡tica completa.  
âœ”ï¸ Buena opciÃ³n para datasets muy grandes.  

### â— **Desventajas**  
âŒ Puede generar un esquema incompleto si el muestreo no es representativo.  
âŒ Si hay datos atÃ­picos o errores en filas fuera del muestreo, el esquema puede ser incorrecto.  

> ğŸ” **RecomendaciÃ³n:** Ajusta el valor de `samplingRatio` en funciÃ³n del tamaÃ±o del dataset (valores recomendados entre **0.1** y **0.3**).

---

## ğŸ”¹ **ComparaciÃ³n de MÃ©todos para Inferir el Esquema**

| MÃ©todo                    | Eficiencia | PrecisiÃ³n | Escenarios Recomendados |
|---------------------------|-------------|------------|---------------------------|
| **Inferencia AutomÃ¡tica**   | ğŸŸ  Medio     | ğŸŸ  Medio    | Datasets pequeÃ±os o medianos |
| **EspecificaciÃ³n Manual**   | ğŸŸ¢ Alta      | ğŸŸ¢ Alta     | Datasets grandes o esquemas conocidos |
| **Muestreo (`samplingRatio`)** | ğŸŸ¢ Alta | ğŸŸ  Medio    | Datasets muy grandes con estructura desconocida |

---

## ğŸ”¹ **Mejores PrÃ¡cticas para Manejar Esquemas en Spark**

âœ… Si el dataset es **grande**, utiliza **definiciÃ³n manual del esquema** para maximizar el rendimiento.  
âœ… Si el dataset es **desconocido** o varÃ­a con el tiempo, usa el **muestreo** para acelerar la inferencia.  
âœ… Si el dataset es **pequeÃ±o o mediano**, la **inferencia automÃ¡tica** es la opciÃ³n mÃ¡s sencilla.  
âœ… Utiliza siempre la funciÃ³n **`sdf_schema()`** despuÃ©s de cargar el dataset para verificar que el esquema sea el esperado.  

---

## ğŸ”¹ **Ejemplo Final: CombinaciÃ³n de MÃ©todos para MÃ¡xima Eficiencia**
La mejor estrategia para datasets masivos consiste en:

1. Utilizar la **inferencia mediante muestreo** en la fase exploratoria para determinar el esquema correcto.  
2. Luego, definir manualmente el esquema para mejorar la eficiencia en el procesamiento final.  

**Ejemplo combinado:**
```r
# 1. Usar sampling para identificar el esquema correcto
taxi_nyc_sample <- spark_read_csv(
  sc, 
  name = "taxi_nyc_sample", 
  path = "hdfs://ruta/dataset.csv",
  infer_schema = TRUE,
  options = list(samplingRatio = 0.1)
)

# 2. Extraer el esquema identificado
schema_identificado <- sdf_schema(taxi_nyc_sample)

# 3. Aplicar el esquema manualmente para mejorar la eficiencia
taxi_nyc <- spark_read_csv(
  sc, 
  name = "taxi_nyc", 
  path = "hdfs://ruta/dataset.csv", 
  columns = schema_identificado
)
```

---


## ğŸ”¹ **5. Ejercicios prÃ¡cticos**

### ğŸŸ© **Ejercicio 1 â€“ AnÃ¡lisis de tarifas limpias usando ETL**
- Carga el dataset de taxis en formato **CSV**.
- Filtra los registros con tarifas negativas o nulas.
- Calcula la tarifa media y la tarifa mÃ¡xima.
- Almacena el resultado en formato **Parquet**.

```r

```

---

### ğŸŸ© **Ejercicio 2 â€“ AnÃ¡lisis de viajes por hora usando ELT**
- Carga el dataset de taxis en formato **Parquet**.
- Crea una columna que extraiga la **hora** de recogida (`pickup_datetime`).
- Calcula el total de viajes por hora.
- Almacena el resultado en formato **Parquet**.


```r

```
