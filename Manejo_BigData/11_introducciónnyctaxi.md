## ğŸ“Œ **Â¿QuÃ© es Big Data y por quÃ© gestionarlo?**

**Big Data** se refiere al manejo y anÃ¡lisis de volÃºmenes extremadamente grandes de datos, que superan las capacidades de las herramientas tradicionales. Este fenÃ³meno se caracteriza comÃºnmente mediante el modelo de las **"5 V"**:

- **Volumen:** La cantidad de datos generados es enorme (terabytes o petabytes).
- **Velocidad:** Los datos se generan de manera continua y muy rÃ¡pida (datos en tiempo real o streaming).
- **Variedad:** Los datos pueden ser estructurados (bases de datos), semi-estructurados (JSON, XML) y no estructurados (texto, imÃ¡genes, videos).
- **Veracidad:** Los datos pueden tener calidad variable o inconsistencias que requieren validaciÃ³n y limpieza.
- **Valor:** Capacidad de extraer informaciÃ³n Ãºtil y estratÃ©gica para tomar decisiones.

---

## ğŸ“Œ **Â¿Por quÃ© las empresas necesitan Big Data?**

Las empresas actuales generan grandes cantidades de informaciÃ³n procedente de mÃºltiples fuentes, como sistemas transaccionales, redes sociales, sensores IoT, etc. Gestionar eficientemente estos datos proporciona ventajas estratÃ©gicas clave:

- **AnÃ¡lisis predictivo avanzado:** Por ejemplo, predecir demanda de productos, identificar fraudes o anticipar la rotaciÃ³n de clientes.
- **Toma de decisiones basada en datos:** Permite decisiones rÃ¡pidas e informadas, mejorando resultados comerciales y operativos.
- **OptimizaciÃ³n de procesos:** Desde rutas logÃ­sticas hasta la gestiÃ³n del inventario en tiempo real.

---

## ğŸ“Œ **Diferencias entre Big Data y manejo tradicional**

| CaracterÃ­stica | Manejo Tradicional | Big Data |
|----------------|--------------------|----------|
| Volumen de datos | Gigabytes | Terabytes o Petabytes |
| Velocidad | Moderada o baja | Muy alta, en tiempo real |
| Variedad | Datos estructurados | Datos estructurados y no estructurados |
| Herramientas | Bases SQL tradicionales | Hadoop, Spark, NoSQL |
| Almacenamiento | Centralizado | Distribuido (HDFS, S3, Cloud Storage) |

---

## ğŸ“Œ **Ejemplo prÃ¡ctico introductorio: NYC Taxi dataset**

El dataset pÃºblico de taxis de Nueva York contiene informaciÃ³n detallada sobre millones de viajes en taxi cada mes, incluyendo aspectos como punto de recogida, destino, duraciÃ³n, costos y propinas. Este dataset cumple perfectamente con las caracterÃ­sticas Big Data:

- **Volumen:** Millones de registros por mes.
- **Velocidad:** Continuamente actualizado cada mes con nuevos datos.
- **Variedad:** Incluye datos estructurados (costos, duraciÃ³n), semi-estructurados (ubicaciones GPS), e incluso texto (metadatos, comentarios).
- **Veracidad:** Puede contener errores, datos faltantes o inconsistentes.
- **Valor:** Ãštil para anÃ¡lisis predictivos, optimizaciÃ³n de tarifas, rutas, y anÃ¡lisis del comportamiento de usuarios.

---

## ğŸ”¹ **DescripciÃ³n del Dataset**

Este dataset es ofrecido pÃºblicamente por la [New York City Taxi and Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page), que regula y publica informaciÃ³n mensual sobre millones de viajes realizados por taxis amarillos en la ciudad de Nueva York.

El dataset incluye informaciÃ³n de cada viaje realizado, como:

- **pickup_datetime y dropoff_datetime**: fecha y hora de inicio y fin del viaje.
- **pickup y drop-off locations**: ubicaciones geogrÃ¡ficas de recogida y llegada.
- **passenger_count**: NÃºmero de pasajeros.
- **trip_distance**: Distancia recorrida durante el viaje (en millas).
- **fare_amount**: Tarifa bÃ¡sica del viaje.
- **tip_amount**: Cantidad de propina dada por el cliente.
- **payment_type**: Tipo de pago (efectivo, tarjeta, etc.).

---

## ğŸ”¹ **Â¿Por quÃ© utilizamos este dataset?**

El dataset cumple con las caracterÃ­sticas principales del Big Data:

- **Volumen**: Millones de registros (entre 8-10 millones por mes aproximadamente).
- **Velocidad**: GeneraciÃ³n continua mensual (actualizaciÃ³n constante).
- **Variedad**: Incluye datos estructurados, y adicionalmente podemos complementarlo con datos semi-estructurados o no estructurados (comentarios de usuarios, eventos externos, etc.).
- **Veracidad**: Ofrece datos pÃºblicos regulados y fiables, aunque siempre puede requerir tareas adicionales de limpieza.

---

## ğŸ”¹ **Enlace para descargar los datos (ejemplo mes de Enero 2023)**

Puedes descargar el dataset de taxis de Enero de 2023 desde aquÃ­ en formato Parquet (formato eficiente para Big Data):

- ğŸ”— [Yellow Taxi Trip Data â€“ Enero 2023 (Parquet)](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet)

*Nota*: Mensualmente se actualiza el dataset en la pÃ¡gina oficial de la TLC [enlace oficial aquÃ­](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).

Puedes descargar el dataset completo directamente desde:

- PÃ¡gina oficial de TLC NYC:
    https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page

- Enlaces directos para el curso (Enero 2023):
  - Formato CSV (~600 MB): https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.csv
  - Formato Parquet (~150 MB, comprimido y eficiente para Big Data): https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet


---

## ğŸ”¹ **Acceso inicial con R y Apache Spark (sparklyr)**

Veamos cÃ³mo cargar rÃ¡pidamente estos datos en Spark usando **`sparklyr` desde R**:

```r
# Cargar librerÃ­as
library(sparklyr)
library(dplyr)

# ConexiÃ³n a Apache Spark local
sc <- spark_connect(master = "local")

# Cargar el dataset de taxis NYC desde URL (parquet)
taxi_nyc <- spark_read_parquet(sc, 
                               name = "taxi_nyc",
                               path = "https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet")

# Mostrar estructura bÃ¡sica
glimpse(taxi_nyc)

# Visualizar primeras filas
head(taxi_nyc)
```

---

## ğŸ”¹ **Ejemplos prÃ¡cticos y usos empresariales reales del dataset**

Este dataset se utiliza frecuentemente en entornos empresariales para:

- **Predecir demanda en transporte urbano**: Ajustar dinÃ¡micamente precios segÃºn demanda y horario.
- **Analizar patrones de trÃ¡fico**: Optimizar rutas, mejorando tiempos de trayecto y disminuyendo costos operativos.
- **Identificar clientes mÃ¡s rentables**: Perfilamiento para programas de fidelizaciÃ³n y marketing dirigido.
- **DetecciÃ³n de anomalÃ­as**: Fraudes, viajes irregulares, posibles errores en tarifas, etc.

---

# ğŸ” **`collect()` en Apache Spark con `sparklyr` en R**  

El mÃ©todo **`collect()`** es una funciÃ³n **clave** en el ecosistema de **Apache Spark**, especialmente en el contexto del anÃ¡lisis de datos masivos utilizando R y la librerÃ­a **`sparklyr`**. Su uso es crucial para entender cÃ³mo se manejan los datos en entornos distribuidos.

---

## ğŸ”¹ **Â¿QuÃ© es `collect()`?**  
En Apache Spark, el mÃ©todo **`collect()`** se utiliza para **extraer datos desde un clÃºster de Spark hacia el entorno local de R**. En otras palabras, **`collect()`** convierte los datos almacenados en Spark (en un entorno distribuido) en un **data frame** estÃ¡ndar de R, permitiendo su manipulaciÃ³n, anÃ¡lisis y visualizaciÃ³n dentro de R.

---

## ğŸ”¹ **Â¿Por quÃ© se necesita `collect()` en Spark?**

El diseÃ±o de **Spark** se basa en el principio de **computaciÃ³n distribuida**, lo que significa que los datos se encuentran repartidos en mÃºltiples nodos del clÃºster.  

Por esta razÃ³n:  
âœ… Cuando ejecutas una operaciÃ³n en Spark (como `filter()`, `group_by()`, etc.), el resultado se mantiene en el entorno de Spark y **no se transfiere automÃ¡ticamente** a R.  
âœ… La funciÃ³n **`collect()`** se usa para **transferir los resultados a la memoria local** en R.  

---

## ğŸ”¹ **Â¿CÃ³mo funciona `collect()`?**

La funciÃ³n **`collect()`** realiza lo siguiente:

âœ… EnvÃ­a una solicitud a Spark para recopilar los resultados de todas las particiones del clÃºster.  
âœ… Los datos se trasladan al entorno local de R como un **data frame estÃ¡ndar**.  
âœ… Es ideal para obtener subconjuntos de datos pequeÃ±os o resultados finales que puedan analizarse directamente en R.  

> âš ï¸ **Â¡Importante!**  
> Dado que `collect()` transfiere los datos completos a la memoria local, no es recomendable para conjuntos de datos muy grandes, ya que puede agotar la memoria del sistema.

---

## ğŸ”¹ **Sintaxis bÃ¡sica de `collect()`**
```r
library(sparklyr)
library(dplyr)

# Conectar a Spark
sc <- spark_connect(master = "local")

# Cargar un dataset en Spark
taxi_nyc <- spark_read_parquet(sc, name = "taxi_nyc", path = "hdfs://ruta/dataset.parquet")

# Filtrar los datos y luego recogerlos en R
datos_filtrados <- taxi_nyc %>%
  filter(fare_amount > 10) %>%
  select(pickup_datetime, fare_amount) %>%
  collect()

# Ver el resultado
head(datos_filtrados)
```

---

## ğŸ”¹ **Ejemplos PrÃ¡cticos de `collect()`**

### ğŸ“‹ **1. Obtener un subconjunto de datos**
El uso mÃ¡s comÃºn de `collect()` es extraer un pequeÃ±o subconjunto de datos para su inspecciÃ³n.

```r
# Obtener 100 registros aleatorios del dataset
sample_data <- taxi_nyc %>%
  sample_n(100) %>%
  collect()

# VisualizaciÃ³n rÃ¡pida
head(sample_data)
```

---

### ğŸ“‹ **2. Agregar resultados resumidos**
En lugar de traer todos los registros, podemos calcular un resumen y luego usar `collect()` para transferir el resultado.

```r
# Tarifa promedio por tipo de pago
tarifa_promedio <- taxi_nyc %>%
  group_by(payment_type) %>%
  summarise(avg_fare = mean(fare_amount, na.rm = TRUE)) %>%
  collect()

# VisualizaciÃ³n del resumen
print(tarifa_promedio)
```

---

### ğŸ“‹ **3. VisualizaciÃ³n directa de resultados**
Como Spark no genera grÃ¡ficos directamente, una estrategia eficaz es:

1. Realizar la transformaciÃ³n en Spark.  
2. Recoger el resultado en R con `collect()`.  
3. Utilizar herramientas como `ggplot2` para graficar.  

---

### ğŸ“‹ **4. Combinar `collect()` con `arrange()`**
Para obtener un listado ordenado desde Spark.

```r
# Top 10 viajes mÃ¡s costosos
viajes_costosos <- taxi_nyc %>%
  arrange(desc(fare_amount)) %>%
  select(pickup_datetime, fare_amount) %>%
  head(10) %>%
  collect()

print(viajes_costosos)
```

---

## ğŸ”¹ **Buenas PrÃ¡cticas al usar `collect()`**

âœ… **Filtra primero, luego usa `collect()`** â†’ Evita traer datos innecesarios.  
âœ… **Usa `head()` antes de `collect()`** para obtener solo una muestra pequeÃ±a.  
âœ… **AsegÃºrate de no ejecutar `collect()` en datasets masivos** sin aplicar previamente filtros o agregaciones.  
âœ… **Prefiere `compute()`** en lugar de `collect()` si deseas crear un nuevo dataset en Spark sin descargarlo.

---

## ğŸ”¹ **Alternativa a `collect()`: `compute()`**

La funciÃ³n **`compute()`** crea un **dataset temporal en Spark**, lo que resulta muy Ãºtil cuando se desea almacenar resultados parciales en el entorno distribuido sin traerlos a R.

**Ejemplo:**
```r
# Guardar un dataset temporal en Spark
datos_temporales <- taxi_nyc %>%
  filter(fare_amount > 50) %>%
  compute(name = "viajes_costosos")

# Visualizar directamente desde Spark sin descargar
datos_temporales %>%
  summarise(total_viajes = n()) %>%
  collect()
```

---

## ğŸ”¹ **Â¿CuÃ¡ndo usar `collect()` y cuÃ¡ndo `compute()`?**

| CaracterÃ­stica   | `collect()`                         | `compute()`                         |
|------------------|-------------------------------------|-------------------------------------|
| **FunciÃ³n principal** | Extrae los datos al entorno local de R  | Crea un dataset temporal dentro de Spark  |
| **Uso recomendado** | Para conjuntos de datos **pequeÃ±os** o resultados finales | Para conjuntos de datos **grandes** que se seguirÃ¡n procesando en Spark |
| **Ventaja clave** | Permite anÃ¡lisis y visualizaciÃ³n directa en R  | Mantiene los datos en el entorno distribuido de Spark |
| **Riesgo** | Puede agotar la memoria si se usa en conjuntos de datos grandes | No sobrecarga la memoria local |

---

> ğŸ§  **Regla de oro:** Filtra, agrega o resume en Spark antes de usar `collect()` para evitar problemas de memoria. ğŸš€

---

# ğŸš€ **Funciones Clave para la GestiÃ³n de Memoria y el Procesamiento Distribuido en Apache Spark con `sparklyr`**

Cuando trabajamos con grandes volÃºmenes de datos en **Apache Spark**, una gestiÃ³n eficiente de la memoria es crucial para garantizar el rendimiento y evitar errores por falta de recursos. AdemÃ¡s, comprender las funciones clave para el **procesamiento distribuido** en Spark ayuda a optimizar los flujos de trabajo y aprovechar al mÃ¡ximo su arquitectura escalable.

En esta guÃ­a, te mostrarÃ© las principales funciones que optimizan el uso de memoria, el manejo de datos en entornos distribuidos y la mejora del rendimiento en Spark usando **`sparklyr`** en R.

---

## ğŸ”¹ **1. `compute()` â€“ Crear un Dataset Temporal en Spark**
La funciÃ³n **`compute()`** es una alternativa eficiente a `collect()` cuando se necesita almacenar un dataset temporal dentro de Spark sin mover los datos al entorno local de R.

### ğŸ” **Â¿Por quÃ© usar `compute()`?**
âœ… Permite guardar el resultado de una operaciÃ³n en memoria dentro del clÃºster de Spark.  
âœ… Evita la transferencia de datos masivos a R.  
âœ… Ãštil cuando se desean reutilizar resultados en varias operaciones posteriores.  

### ğŸ” **Ejemplo de uso**
```r
# Crear un dataset temporal en Spark
datos_temporales <- taxi_nyc %>%
  filter(fare_amount > 20) %>%
  compute(name = "datos_temporales")

# Consultar los registros directamente en Spark
datos_temporales %>%
  summarise(total_viajes = n()) %>%
  collect()
```

---

## ğŸ”¹ **2. `persist()` â€“ Persistir Datos en Memoria o Disco**
La funciÃ³n **`persist()`** permite almacenar datos en memoria (RAM), en disco o en ambos. Es ideal para almacenar datasets intermedios que se reutilizarÃ¡n varias veces.

### ğŸ” **Â¿Por quÃ© usar `persist()`?**
âœ… Mejora el rendimiento evitando que Spark tenga que recalcular datos repetidamente.  
âœ… Se puede elegir el nivel de almacenamiento:  

- **`"MEMORY_ONLY"`** â†’ Almacena solo en memoria (rÃ¡pido, pero puede agotar recursos).  
- **`"MEMORY_AND_DISK"`** â†’ Combina memoria y disco (recomendado para datasets grandes).  
- **`"DISK_ONLY"`** â†’ Almacena solo en disco (menos eficiente pero seguro).

### ğŸ” **Ejemplo de uso**
```r
# Persistir los datos en memoria para optimizar su reutilizaciÃ³n
taxi_nyc %>%
  filter(trip_distance > 5) %>%
  sdf_persist(storage.level = "MEMORY_ONLY")
```

---

## ğŸ”¹ **3. `cache()` â€“ Almacenar Datos en Memoria**
La funciÃ³n **`cache()`** es similar a `persist()` pero mÃ¡s sencilla. Se recomienda para almacenar datos en memoria que se reutilizarÃ¡n varias veces durante la sesiÃ³n de Spark.

### ğŸ” **Â¿Por quÃ© usar `cache()`?**
âœ… Almacena los datos en memoria RAM para una recuperaciÃ³n rÃ¡pida.  
âœ… Ãštil para conjuntos de datos que se consultarÃ¡n repetidamente.  
âœ… **No es configurable**, por lo que se almacena exclusivamente en memoria.  

> âš ï¸ Si el dataset es muy grande, **`cache()`** puede agotar la memoria del clÃºster.  

### ğŸ” **Ejemplo de uso**
```r
# Cachear un dataset en memoria
taxi_nyc %>%
  filter(fare_amount > 20) %>%
  cache()
```

---

## ğŸ”¹ **4. `unpersist()` â€“ Liberar Memoria en Spark**
La funciÃ³n **`unpersist()`** se utiliza para **liberar los datos almacenados en memoria** o en disco mediante `persist()` o `cache()`.

### ğŸ” **Â¿Por quÃ© usar `unpersist()`?**
âœ… Es importante liberar los datos cuando ya no se necesiten para evitar problemas de memoria.  
âœ… Permite liberar la memoria manualmente en lugar de depender del recolector de basura automÃ¡tico.

### ğŸ” **Ejemplo de uso**
```r
# Eliminar un dataset almacenado en memoria para liberar recursos
taxi_nyc %>%
  unpersist()
```

---

## ğŸ”¹ **5. `repartition()` â€“ Optimizar el Particionado de Datos**
La funciÃ³n **`repartition()`** se utiliza para **redistribuir** los datos en un nÃºmero especÃ­fico de particiones dentro del clÃºster de Spark.

### ğŸ” **Â¿Por quÃ© usar `repartition()`?**
âœ… Permite mejorar la **paralelizaciÃ³n** de las tareas en Spark.  
âœ… Es especialmente Ãºtil cuando se detectan **particiones desbalanceadas**.  
âœ… Recomendado para preparar los datos antes de realizar grandes agregaciones o joins.  

> âš ï¸ **`repartition()`** reorganiza los datos completamente, lo que puede consumir tiempo si se utiliza en exceso.

### ğŸ” **Ejemplo de uso**
```r
# Redistribuir el dataset en 200 particiones
taxi_nyc <- taxi_nyc %>%
  repartition(200)
```

---

## ğŸ”¹ **6. `coalesce()` â€“ Reducir el NÃºmero de Particiones**
La funciÃ³n **`coalesce()`** se utiliza para reducir el nÃºmero de particiones en Spark sin reorganizar completamente los datos. Es mÃ¡s eficiente que `repartition()` para este fin.

### ğŸ” **Â¿Por quÃ© usar `coalesce()`?**
âœ… Menos costosa que `repartition()` porque no redistribuye los datos de forma completa.  
âœ… Ideal cuando se desea reducir el nÃºmero de particiones despuÃ©s de una operaciÃ³n de filtrado o agregaciÃ³n.

### ğŸ” **Ejemplo de uso**
```r
# Reducir el nÃºmero de particiones de 200 a 50
taxi_nyc <- taxi_nyc %>%
  coalesce(50)
```

---

## ğŸ”¹ **7. `group_by()` + `summarise()` â€“ AgregaciÃ³n Eficiente**
El uso combinado de **`group_by()`** y **`summarise()`** permite realizar agregaciones directamente en Spark de forma eficiente.

### ğŸ” **Â¿Por quÃ© usar esta combinaciÃ³n?**
âœ… Permite realizar cÃ¡lculos directamente en el entorno distribuido.  
âœ… Evita mover grandes volÃºmenes de datos al entorno local.  

### ğŸ” **Ejemplo de uso**
```r
# Calcular la tarifa promedio por tipo de pago en Spark
tarifa_promedio <- taxi_nyc %>%
  group_by(payment_type) %>%
  summarise(tarifa_media = mean(fare_amount, na.rm = TRUE)) %>%
  collect()
```

---

## ğŸ”¹ **8. `distinct()` â€“ Eliminar Duplicados en Spark**
La funciÃ³n **`distinct()`** permite identificar y eliminar registros duplicados directamente en el entorno distribuido de Spark.

### ğŸ” **Â¿Por quÃ© usar `distinct()`?**
âœ… Optimiza el rendimiento eliminando duplicados directamente en el clÃºster.  
âœ… Es mÃ¡s eficiente que `unique()` cuando se trabaja con grandes volÃºmenes de datos.  

### ğŸ” **Ejemplo de uso**
```r
# Eliminar duplicados en el dataset
datos_limpios <- taxi_nyc %>%
  distinct(pickup_datetime, dropoff_datetime, trip_distance)
```

---

## ğŸ”¹ **9. `explain()` â€“ AnÃ¡lisis del Plan de EjecuciÃ³n**
La funciÃ³n **`explain()`** permite analizar cÃ³mo Spark estÃ¡ ejecutando una consulta.

### ğŸ” **Â¿Por quÃ© usar `explain()`?**
âœ… Permite identificar posibles problemas de rendimiento.  
âœ… Muestra el **plan de ejecuciÃ³n** que Spark estÃ¡ utilizando.  
âœ… Ãštil para optimizar el flujo de trabajo en Spark.  

### ğŸ” **Ejemplo de uso**
```r
# Visualizar el plan de ejecuciÃ³n para identificar problemas de rendimiento
taxi_nyc %>%
  filter(fare_amount > 50) %>%
  explain()
```

---

## ğŸ”¹ **10. `sdf_sample()` â€“ Muestreo Eficiente en Spark**
La funciÃ³n **`sdf_sample()`** permite extraer muestras representativas de grandes volÃºmenes de datos directamente en Spark.

### ğŸ” **Â¿Por quÃ© usar `sdf_sample()`?**
âœ… Permite explorar datos sin cargar todo el conjunto en memoria.  
âœ… Ideal para realizar anÃ¡lisis exploratorio en conjuntos masivos.  

### ğŸ” **Ejemplo de uso**
```r
# Extraer una muestra del 10% del dataset
muestra <- taxi_nyc %>%
  sdf_sample(fraction = 0.1) %>%
  collect()

head(muestra)
```


---

## ğŸ”¹ **CÃ³digo de exploraciÃ³n bÃ¡sica inicial en R/Spark**

A continuaciÃ³n, exploramos de forma bÃ¡sica algunos aspectos fundamentales del dataset:

```r
# NÃºmero total de registros
taxi_nyc %>% summarise(total_viajes = n()) %>% collect()

# Campos (columnas) disponibles
colnames(taxi_nyc)

# Muestra inicial del dataset
taxi_nyc %>% head(10) %>% collect()
```

---

## ğŸ”¹ **Ejercicios iniciales sugeridos**

Estos ejercicios permiten comenzar la familiarizaciÃ³n inicial con el dataset:

**Ejercicio 1:**  
- Cargar datos usando sparklyr (ver secciÃ³n previa)
- Explorar columnas y tipos de datos

```r
taxi_nyc %>% glimpse()
```

**Ejercicio 2:**  
- Determinar tarifa promedio, propina promedio y distancia promedio de todos los viajes.

```r
taxi_nyc %>% summarise(
  tarifa_media = mean(fare_amount, na.rm=TRUE),
  propina_media = mean(tip_amount, na.rm=TRUE),
  distancia_media = mean(trip_distance, na.rm=TRUE)
) %>% collect()
```

**Ejercicio 3 â€“ AnÃ¡lisis de Propinas Elevadas**
**Objetivo:**  
- Filtra los viajes con propinas (`tip_amount`) superiores a **10 USD**.  
- Devuelve las 10 propinas mÃ¡s altas junto con la fecha y hora del viaje.  

**CÃ³digo**
```r
```

---

## ğŸ“Œ **Ejercicios propuestos**

AquÃ­ tienes dos ejercicios introductorios para poner en prÃ¡ctica los conceptos explicados usando el dataset de NYC Taxis:

### ğŸŸ© **Ejercicio 1 â€“ ExploraciÃ³n inicial del dataset**

Carga el dataset usando Spark en R, explora los datos iniciales, e identifica:

- Â¿CuÃ¡ntos registros tiene el dataset?
- Â¿QuÃ© campos (columnas) estÃ¡n disponibles?
- Â¿QuÃ© tipo de datos predominan (estructurados, semi-estructurados)?


```r

```

---

### ğŸŸ© **Ejercicio 2 â€“ AnÃ¡lisis bÃ¡sico del comportamiento de los pasajeros**

Utilizando la informaciÃ³n cargada anteriormente, responde:

- Â¿CuÃ¡l es la tarifa promedio (`fare_amount`) de los viajes realizados en enero de 2023?
- Â¿CuÃ¡l es la propina promedio (`tip_amount`)?
- Â¿QuÃ© porcentaje de viajes pagÃ³ propina superior a $5?


```r

```

